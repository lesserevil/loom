id: bd-run-failure-20260120033907
type: task
title: "P0 - run failed"
description: |
  Command: docker compose up --build
  Exit code: 130

  Output:
     Image agenticorp:latest Building 
     Image agenticorp-agenticorp-test Building 
    #1 [internal] load local bake definitions
    #1 reading from stdin 1.00kB done
    #1 DONE 0.0s
    
    #2 [agenticorp internal] load build definition from Dockerfile
    #2 DONE 0.0s
    
    #2 [agenticorp internal] load build definition from Dockerfile
    #2 transferring dockerfile: 1.23kB done
    #2 DONE 0.0s
    
    #3 [auth] library/golang:pull token for registry-1.docker.io
    #3 DONE 0.0s
    
    #4 [auth] library/alpine:pull token for registry-1.docker.io
    #4 DONE 0.0s
    
    #5 [agenticorp-test internal] load metadata for docker.io/library/golang:1.24-alpine
    #5 DONE 0.9s
    
    #6 [agenticorp internal] load metadata for docker.io/library/alpine:latest
    #6 DONE 0.9s
    
    #7 [agenticorp-test internal] load .dockerignore
    #7 transferring context: 353B done
    #7 DONE 0.0s
    
    #8 [agenticorp stage-1 1/8] FROM docker.io/library/alpine:latest@sha256:865b95f46d98cf867a156fe4a135ad3fe50d2056aa3f25ed31662dff6da4eb62
    #8 resolve docker.io/library/alpine:latest@sha256:865b95f46d98cf867a156fe4a135ad3fe50d2056aa3f25ed31662dff6da4eb62 0.0s done
    #8 DONE 0.0s
    
    #9 [agenticorp-test builder 1/8] FROM docker.io/library/golang:1.24-alpine@sha256:598dbb5f4658bd66f3ad7bfda86a9af580812b57065d8854add882e64480d986
    #9 resolve docker.io/library/golang:1.24-alpine@sha256:598dbb5f4658bd66f3ad7bfda86a9af580812b57065d8854add882e64480d986 0.0s done
    #9 DONE 0.0s
    
    #10 [agenticorp internal] load build context
    #10 transferring context: 12.63kB 0.0s done
    #10 DONE 0.0s
    
    #11 [agenticorp builder 7/8] COPY . .
    #11 CACHED
    
    #12 [agenticorp builder 5/8] COPY go.sum ./
    #12 CACHED
    
    #13 [agenticorp builder 2/8] RUN apk add --no-cache git ca-certificates tzdata gcc musl-dev
    #13 CACHED
    
    #14 [agenticorp builder 3/8] WORKDIR /build
    #14 CACHED
    
    #15 [agenticorp builder 6/8] RUN go mod download
    #15 CACHED
    
    #16 [agenticorp builder 4/8] COPY go.mod ./
    #16 CACHED
    
    #17 [agenticorp stage-1 4/8] WORKDIR /app
    #17 CACHED
    
    #18 [agenticorp stage-1 6/8] COPY --from=builder /build/config.yaml /app/config.yaml
    #18 CACHED
    
    #19 [agenticorp stage-1 5/8] COPY --from=builder /build/agenticorp /app/agenticorp
    #19 CACHED
    
    #20 [agenticorp stage-1 3/8] RUN addgroup -g 1000 agenticorp &&     adduser -D -u 1000 -G agenticorp agenticorp
    #20 CACHED
    
    #21 [agenticorp stage-1 7/8] COPY --from=builder /build/personas /app/personas
    #21 CACHED
    
    #22 [agenticorp stage-1 2/8] RUN apk add --no-cache ca-certificates tzdata
    #22 CACHED
    
    #23 [agenticorp stage-1 8/8] RUN chown -R agenticorp:agenticorp /app
    #23 CACHED
    
    #24 [agenticorp builder 8/8] RUN CGO_ENABLED=1 GOOS=linux go build     -ldflags="-w -s"     -o agenticorp     ./cmd/agenticorp
    #24 CACHED
    
    #25 [agenticorp] exporting to image
    #25 exporting layers done
    #25 exporting manifest sha256:188fc719a76c01d382838a9226f68a3f7ef0fa0c9ccb30b727b6b085af56d854 done
    #25 exporting config sha256:97c77abb887533e22cf4c217fc22d41be43e11fb732aed3ed6362728fff6e673 done
    #25 exporting attestation manifest sha256:dd580d78ba775877162b27705b821796e2768c5aeb4b731a4a3a73b11c2229f3 0.0s done
    #25 exporting manifest list sha256:ea1b0631d0c2bd08e123cab955770f851f0676a756186dfbec391ea47bfc559c done
    #25 naming to docker.io/library/agenticorp:latest done
    #25 unpacking to docker.io/library/agenticorp:latest done
    #25 DONE 0.1s
    
    #26 [agenticorp-test] exporting to image
    #26 exporting layers done
    #26 exporting manifest sha256:031222cdae4128289b523053c0f82363c5bf63c9ab1b378ba509bf79aa7dc387 done
    #26 exporting config sha256:a6532e5ff4d3263ee1f7599de1d88165ba2c7ae2d3441c17806e293ccba0f233 done
    #26 exporting attestation manifest sha256:459d287e7fa50f89851988da8320bc662cc4f38fe90b6952dd08b56438cab5a4 0.0s done
    #26 exporting manifest list sha256:3de5b3fd9587c027d7aeb1315be1a50a69e66580a4341265a5d8cccc91b2adb6 done
    #26 naming to docker.io/library/agenticorp-agenticorp-test:latest done
    #26 unpacking to docker.io/library/agenticorp-agenticorp-test:latest done
    #26 DONE 0.1s
    
    #27 [agenticorp-test] resolving provenance for metadata file
    #27 DONE 0.0s
    
    #28 [agenticorp] resolving provenance for metadata file
    #28 DONE 0.0s
     Image agenticorp:latest Built 
     Image agenticorp-agenticorp-test Built 
     Network agenticorp_agenticorp-network Creating 
     Network agenticorp_agenticorp-network Created 
     Container temporal-postgresql Creating 
     Container temporal-postgresql Created 
     Container temporal Creating 
     Container temporal Created 
     Container temporal-ui Creating 
     Container agenticorp Creating 
     Container agenticorp-test Creating 
     Container temporal-ui Created 
     Container agenticorp Created 
     Container agenticorp-test Created 
    Attaching to agenticorp, agenticorp-test, temporal, temporal-postgresql, temporal-ui
     Container temporal-postgresql Starting 
     Container temporal-postgresql Started 
     Container temporal-postgresql Waiting 
    temporal-postgresql  | The files belonging to this database system will be owned by user "postgres".
    temporal-postgresql  | This user must also own the server process.
    temporal-postgresql  | 
    temporal-postgresql  | The database cluster will be initialized with locale "en_US.utf8".
    temporal-postgresql  | The default database encoding has accordingly been set to "UTF8".
    temporal-postgresql  | The default text search configuration will be set to "english".
    temporal-postgresql  | 
    temporal-postgresql  | Data page checksums are disabled.
    temporal-postgresql  | 
    temporal-postgresql  | fixing permissions on existing directory /var/lib/postgresql/data ... ok
    temporal-postgresql  | creating subdirectories ... ok
    temporal-postgresql  | selecting dynamic shared memory implementation ... posix
    temporal-postgresql  | selecting default max_connections ... 100
    temporal-postgresql  | selecting default shared_buffers ... 128MB
    temporal-postgresql  | selecting default time zone ... UTC
    temporal-postgresql  | creating configuration files ... ok
    temporal-postgresql  | running bootstrap script ... ok
    temporal-postgresql  | sh: locale: not found
    temporal-postgresql  | 2026-01-20 03:18:05.634 UTC [34] WARNING:  no usable system locales were found
    temporal-postgresql  | performing post-bootstrap initialization ... ok
    temporal-postgresql  | syncing data to disk ... ok
    temporal-postgresql  | 
    temporal-postgresql  | 
    temporal-postgresql  | Success. You can now start the database server using:
    temporal-postgresql  | 
    temporal-postgresql  |     pg_ctl -D /var/lib/postgresql/data -l logfile start
    temporal-postgresql  | 
    temporal-postgresql  | initdb: warning: enabling "trust" authentication for local connections
    temporal-postgresql  | initdb: hint: You can change this by editing pg_hba.conf or using the option -A, or --auth-local and --auth-host, the next time you run initdb.
    temporal-postgresql  | waiting for server to start....2026-01-20 03:18:05.907 UTC [40] LOG:  starting PostgreSQL 15.15 on aarch64-unknown-linux-musl, compiled by gcc (Alpine 15.2.0) 15.2.0, 64-bit
    temporal-postgresql  | 2026-01-20 03:18:05.908 UTC [40] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
    temporal-postgresql  | 2026-01-20 03:18:05.909 UTC [43] LOG:  database system was shut down at 2026-01-20 03:18:05 UTC
    temporal-postgresql  | 2026-01-20 03:18:05.911 UTC [40] LOG:  database system is ready to accept connections
    temporal-postgresql  |  done
    temporal-postgresql  | server started
    temporal-postgresql  | CREATE DATABASE
    temporal-postgresql  | 
    temporal-postgresql  | 
    temporal-postgresql  | /usr/local/bin/docker-entrypoint.sh: ignoring /docker-entrypoint-initdb.d/*
    temporal-postgresql  | 
    temporal-postgresql  | waiting for server to shut down....2026-01-20 03:18:06.041 UTC [40] LOG:  received fast shutdown request
    temporal-postgresql  | 2026-01-20 03:18:06.043 UTC [40] LOG:  aborting any active transactions
    temporal-postgresql  | 2026-01-20 03:18:06.044 UTC [40] LOG:  background worker "logical replication launcher" (PID 46) exited with exit code 1
    temporal-postgresql  | 2026-01-20 03:18:06.044 UTC [41] LOG:  shutting down
    temporal-postgresql  | 2026-01-20 03:18:06.045 UTC [41] LOG:  checkpoint starting: shutdown immediate
    temporal-postgresql  | 2026-01-20 03:18:06.068 UTC [41] LOG:  checkpoint complete: wrote 921 buffers (5.6%); 0 WAL file(s) added, 0 removed, 0 recycled; write=0.008 s, sync=0.014 s, total=0.024 s; sync files=301, longest=0.006 s, average=0.001 s; distance=4238 kB, estimate=4238 kB
    temporal-postgresql  | 2026-01-20 03:18:06.070 UTC [40] LOG:  database system is shut down
    temporal-postgresql  |  done
    temporal-postgresql  | server stopped
    temporal-postgresql  | 
    temporal-postgresql  | PostgreSQL init process complete; ready for start up.
    temporal-postgresql  | 
    temporal-postgresql  | 2026-01-20 03:18:06.166 UTC [1] LOG:  starting PostgreSQL 15.15 on aarch64-unknown-linux-musl, compiled by gcc (Alpine 15.2.0) 15.2.0, 64-bit
    temporal-postgresql  | 2026-01-20 03:18:06.166 UTC [1] LOG:  listening on IPv4 address "0.0.0.0", port 5432
    temporal-postgresql  | 2026-01-20 03:18:06.166 UTC [1] LOG:  listening on IPv6 address "::", port 5432
    temporal-postgresql  | 2026-01-20 03:18:06.168 UTC [1] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
    temporal-postgresql  | 2026-01-20 03:18:06.170 UTC [56] LOG:  database system was shut down at 2026-01-20 03:18:06 UTC
    temporal-postgresql  | 2026-01-20 03:18:06.173 UTC [1] LOG:  database system is ready to accept connections
     Container temporal-postgresql Healthy 
     Container temporal Starting 
    temporal             | TEMPORAL_ADDRESS is not set, setting it to 172.18.0.3:7233
     Container temporal Started 
     Container temporal Waiting 
     Container temporal Waiting 
     Container temporal Waiting 
    temporal             | PostgreSQL started.
    temporal             | Setup PostgreSQL schema.
    temporal             | 2026-01-20T03:18:16.039Z[34mINFO[0mStarting schema setup{"config": {"SchemaFilePath":"","InitialVersion":"0.0","Overwrite":false,"DisableVersioning":false}, "logging-call-at": "setuptask.go:58"}
    temporal             | 2026-01-20T03:18:16.039Z[35mDEBUG[0mSetting up version tables{"logging-call-at": "setuptask.go:68"}
    temporal             | 2026-01-20T03:18:16.045Z[35mDEBUG[0mSetting initial schema version to 0.0{"logging-call-at": "setuptask.go:119"}
    temporal             | 2026-01-20T03:18:16.046Z[35mDEBUG[0mUpdating schema update log{"logging-call-at": "setuptask.go:124"}
    temporal             | 2026-01-20T03:18:16.046Z[34mINFO[0mSchema setup complete{"logging-call-at": "setuptask.go:132"}
    temporal             | 2026-01-20T03:18:16.060Z[34mINFO[0mUpdateSchemeTask started{"config": {"DBName":"","TargetVersion":"","SchemaDir":"/etc/temporal/schema/postgresql/v96/temporal/versioned","IsDryRun":false}, "logging-call-at": "updatetask.go:97"}
    temporal             | 2026-01-20T03:18:16.062Z[35mDEBUG[0mSchema Dirs: [v1.0 v1.1 v1.2 v1.3 v1.4 v1.5 v1.6 v1.7 v1.8 v1.9 v1.10]{"logging-call-at": "updatetask.go:195"}
    temporal             | 2026-01-20T03:18:16.062Z[34mINFO[0mProcessing schema file: /etc/temporal/schema/postgresql/v96/temporal/versioned/v1.0/schema.sql{"logging-call-at": "updatetask.go:241"}
    temporal             | 2026-01-20T03:18:16.062Z[34mINFO[0mProcessing schema file: /etc/temporal/schema/postgresql/v96/temporal/versioned/v1.1/cluster_metadata.sql{"logging-call-at": "updatetask.go:241"}
    temporal             | 2026-01-20T03:18:16.062Z[34mINFO[0mProcessing schema file: /etc/temporal/schema/postgresql/v96/temporal/versioned/v1.2/queue.sql{"logging-call-at": "updatetask.go:241"}
    temporal             | 2026-01-20T03:18:16.063Z[34mINFO[0mProcessing schema file: /etc/temporal/schema/postgresql/v96/temporal/versioned/v1.3/visibility_tasks.sql{"logging-call-at": "updatetask.go:241"}
    temporal             | 2026-01-20T03:18:16.063Z[34mINFO[0mProcessing schema file: /etc/temporal/schema/postgresql/v96/temporal/versioned/v1.4/cluster_metadata.sql{"logging-call-at": "updatetask.go:241"}
    temporal             | 2026-01-20T03:18:16.063Z[34mINFO[0mProcessing schema file: /etc/temporal/schema/postgresql/v96/temporal/versioned/v1.5/event.sql{"logging-call-at": "updatetask.go:241"}
    temporal             | 2026-01-20T03:18:16.063Z[34mINFO[0mProcessing schema file: /etc/temporal/schema/postgresql/v96/temporal/versioned/v1.5/executions.sql{"logging-call-at": "updatetask.go:241"}
    temporal             | 2026-01-20T03:18:16.063Z[34mINFO[0mProcessing schema file: /etc/temporal/schema/postgresql/v96/temporal/versioned/v1.5/cluster_membership.sql{"logging-call-at": "updatetask.go:241"}
    temporal             | 2026-01-20T03:18:16.063Z[34mINFO[0mProcessing schema file: /etc/temporal/schema/postgresql/v96/temporal/versioned/v1.6/queue_metadata.sql{"logging-call-at": "updatetask.go:241"}
    temporal             | 2026-01-20T03:18:16.064Z[34mINFO[0mProcessing schema file: /etc/temporal/schema/postgresql/v96/temporal/versioned/v1.7/cluster_metadata_info.sql{"logging-call-at": "updatetask.go:241"}
    temporal             | 2026-01-20T03:18:16.064Z[34mINFO[0mProcessing schema file: /etc/temporal/schema/postgresql/v96/temporal/versioned/v1.7/no_start_version.sql{"logging-call-at": "updatetask.go:241"}
    temporal             | 2026-01-20T03:18:16.064Z[34mINFO[0mProcessing schema file: /etc/temporal/schema/postgresql/v96/temporal/versioned/v1.7/tiered_storage_tasks.sql{"logging-call-at": "updatetask.go:241"}
    temporal             | 2026-01-20T03:18:16.064Z[34mINFO[0mProcessing schema file: /etc/temporal/schema/postgresql/v96/temporal/versioned/v1.8/drop_unused_tasks_table.sql{"logging-call-at": "updatetask.go:241"}
    temporal             | 2026-01-20T03:18:16.064Z[34mINFO[0mProcessing schema file: /etc/temporal/schema/postgresql/v96/temporal/versioned/v1.8/alter_columns.sql{"logging-call-at": "updatetask.go:241"}
    temporal             | 2026-01-20T03:18:16.064Z[34mINFO[0mProcessing schema file: /etc/temporal/schema/postgresql/v96/temporal/versioned/v1.9/history_tasks_table.sql{"logging-call-at": "updatetask.go:241"}
    temporal             | 2026-01-20T03:18:16.064Z[34mINFO[0mProcessing schema file: /etc/temporal/schema/postgresql/v96/temporal/versioned/v1.10/task_queue_user_data.sql{"logging-call-at": "updatetask.go:241"}
    temporal             | 2026-01-20T03:18:16.064Z[35mDEBUG[0mrunning 11 updates for current version 0.0{"logging-call-at": "updatetask.go:131"}
    temporal             | 2026-01-20T03:18:16.064Z[35mDEBUG[0m---- Executing updates for version 1.0 ----{"logging-call-at": "updatetask.go:150"}
    temporal             | 2026-01-20T03:18:16.064Z[35mDEBUG[0mCREATE TABLE namespaces( partition_id INTEGER NOT NULL, id BYTEA NOT NULL, name VARCHAR(255) UNIQUE NOT NULL, notification_version BIGINT NOT NULL, data BYTEA NOT NULL, data_encoding VARCHAR(16) NOT NULL, is_global BOOLEAN NOT NULL, PRIMARY KEY(partition_id, id) );{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.068Z[35mDEBUG[0mCREATE TABLE namespace_metadata ( partition_id INTEGER NOT NULL, notification_version BIGINT NOT NULL, PRIMARY KEY(partition_id) );{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.069Z[35mDEBUG[0mINSERT INTO namespace_metadata (partition_id, notification_version) VALUES (54321, 1);{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.070Z[35mDEBUG[0mCREATE TABLE shards ( shard_id INTEGER NOT NULL, range_id BIGINT NOT NULL, data BYTEA NOT NULL, data_encoding VARCHAR(16) NOT NULL, PRIMARY KEY (shard_id) );{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.072Z[35mDEBUG[0mCREATE TABLE transfer_tasks( shard_id INTEGER NOT NULL, task_id BIGINT NOT NULL, data BYTEA NOT NULL, data_encoding VARCHAR(16) NOT NULL, PRIMARY KEY (shard_id, task_id) );{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.074Z[35mDEBUG[0mCREATE TABLE executions( shard_id INTEGER NOT NULL, namespace_id BYTEA NOT NULL, workflow_id VARCHAR(255) NOT NULL, run_id BYTEA NOT NULL, next_event_id BIGINT NOT NULL, last_write_version BIGINT NOT NULL, data BYTEA NOT NULL, data_encoding VARCHAR(16) NOT NULL, state BYTEA NOT NULL, state_encoding VARCHAR(16) NOT NULL, PRIMARY KEY (shard_id, namespace_id, workflow_id, run_id) );{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.076Z[35mDEBUG[0mCREATE TABLE current_executions( shard_id INTEGER NOT NULL, namespace_id BYTEA NOT NULL, workflow_id VARCHAR(255) NOT NULL, run_id BYTEA NOT NULL, create_request_id VARCHAR(64) NOT NULL, state INTEGER NOT NULL, status INTEGER NOT NULL, start_version BIGINT NOT NULL, last_write_version BIGINT NOT NULL, PRIMARY KEY (shard_id, namespace_id, workflow_id) );{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.079Z[35mDEBUG[0mCREATE TABLE buffered_events ( shard_id INTEGER NOT NULL, namespace_id BYTEA NOT NULL, workflow_id VARCHAR(255) NOT NULL, run_id BYTEA NOT NULL, id BIGSERIAL NOT NULL UNIQUE, data BYTEA NOT NULL, data_encoding VARCHAR(16) NOT NULL, PRIMARY KEY (shard_id, namespace_id, workflow_id, run_id, id) );{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.082Z[35mDEBUG[0mCREATE TABLE tasks ( range_hash BIGINT NOT NULL, task_queue_id BYTEA NOT NULL, task_id BIGINT NOT NULL, data BYTEA NOT NULL, data_encoding VARCHAR(16) NOT NULL, PRIMARY KEY (range_hash, task_queue_id, task_id) );{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.085Z[35mDEBUG[0mCREATE TABLE task_queues ( range_hash BIGINT NOT NULL, task_queue_id BYTEA NOT NULL, range_id BIGINT NOT NULL, data BYTEA NOT NULL, data_encoding VARCHAR(16) NOT NULL, PRIMARY KEY (range_hash, task_queue_id) );{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.086Z[35mDEBUG[0mCREATE TABLE replication_tasks ( shard_id INTEGER NOT NULL, task_id BIGINT NOT NULL, data BYTEA NOT NULL, data_encoding VARCHAR(16) NOT NULL, PRIMARY KEY (shard_id, task_id) );{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.089Z[35mDEBUG[0mCREATE TABLE replication_tasks_dlq ( source_cluster_name VARCHAR(255) NOT NULL, shard_id INTEGER NOT NULL, task_id BIGINT NOT NULL, data BYTEA NOT NULL, data_encoding VARCHAR(16) NOT NULL, PRIMARY KEY (source_cluster_name, shard_id, task_id) );{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.091Z[35mDEBUG[0mCREATE TABLE timer_tasks ( shard_id INTEGER NOT NULL, visibility_timestamp TIMESTAMP NOT NULL, task_id BIGINT NOT NULL, data BYTEA NOT NULL, data_encoding VARCHAR(16) NOT NULL, PRIMARY KEY (shard_id, visibility_timestamp, task_id) );{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.094Z[35mDEBUG[0mCREATE TABLE activity_info_maps ( shard_id INTEGER NOT NULL, namespace_id BYTEA NOT NULL, workflow_id VARCHAR(255) NOT NULL, run_id BYTEA NOT NULL, schedule_id BIGINT NOT NULL, data BYTEA NOT NULL, data_encoding VARCHAR(16), PRIMARY KEY (shard_id, namespace_id, workflow_id, run_id, schedule_id) );{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.096Z[35mDEBUG[0mCREATE TABLE timer_info_maps ( shard_id INTEGER NOT NULL, namespace_id BYTEA NOT NULL, workflow_id VARCHAR(255) NOT NULL, run_id BYTEA NOT NULL, timer_id VARCHAR(255) NOT NULL, data BYTEA NOT NULL, data_encoding VARCHAR(16), PRIMARY KEY (shard_id, namespace_id, workflow_id, run_id, timer_id) );{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.098Z[35mDEBUG[0mCREATE TABLE child_execution_info_maps ( shard_id INTEGER NOT NULL, namespace_id BYTEA NOT NULL, workflow_id VARCHAR(255) NOT NULL, run_id BYTEA NOT NULL, initiated_id BIGINT NOT NULL, data BYTEA NOT NULL, data_encoding VARCHAR(16), PRIMARY KEY (shard_id, namespace_id, workflow_id, run_id, initiated_id) );{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.100Z[35mDEBUG[0mCREATE TABLE request_cancel_info_maps ( shard_id INTEGER NOT NULL, namespace_id BYTEA NOT NULL, workflow_id VARCHAR(255) NOT NULL, run_id BYTEA NOT NULL, initiated_id BIGINT NOT NULL, data BYTEA NOT NULL, data_encoding VARCHAR(16), PRIMARY KEY (shard_id, namespace_id, workflow_id, run_id, initiated_id) );{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.103Z[35mDEBUG[0mCREATE TABLE signal_info_maps ( shard_id INTEGER NOT NULL, namespace_id BYTEA NOT NULL, workflow_id VARCHAR(255) NOT NULL, run_id BYTEA NOT NULL, initiated_id BIGINT NOT NULL, data BYTEA NOT NULL, data_encoding VARCHAR(16), PRIMARY KEY (shard_id, namespace_id, workflow_id, run_id, initiated_id) );{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.105Z[35mDEBUG[0mCREATE TABLE signals_requested_sets ( shard_id INTEGER NOT NULL, namespace_id BYTEA NOT NULL, workflow_id VARCHAR(255) NOT NULL, run_id BYTEA NOT NULL, signal_id VARCHAR(64) NOT NULL, PRIMARY KEY (shard_id, namespace_id, workflow_id, run_id, signal_id) );{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.108Z[35mDEBUG[0mCREATE TABLE history_node ( shard_id INTEGER NOT NULL, tree_id BYTEA NOT NULL, branch_id BYTEA NOT NULL, node_id BIGINT NOT NULL, txn_id BIGINT NOT NULL, data BYTEA NOT NULL, data_encoding VARCHAR(16) NOT NULL, PRIMARY KEY (shard_id, tree_id, branch_id, node_id, txn_id) );{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.112Z[35mDEBUG[0mCREATE TABLE history_tree ( shard_id INTEGER NOT NULL, tree_id BYTEA NOT NULL, branch_id BYTEA NOT NULL, data BYTEA NOT NULL, data_encoding VARCHAR(16) NOT NULL, PRIMARY KEY (shard_id, tree_id, branch_id) );{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.113Z[35mDEBUG[0mCREATE TABLE queue ( queue_type INTEGER NOT NULL, message_id BIGINT NOT NULL, message_payload BYTEA NOT NULL, PRIMARY KEY(queue_type, message_id) );{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.115Z[35mDEBUG[0mCREATE TABLE queue_metadata ( queue_type INTEGER NOT NULL, data BYTEA NOT NULL, PRIMARY KEY(queue_type) );{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.117Z[35mDEBUG[0mCREATE TABLE cluster_metadata ( metadata_partition INTEGER NOT NULL, immutable_data BYTEA NOT NULL, immutable_data_encoding VARCHAR(16) NOT NULL, PRIMARY KEY(metadata_partition) );{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.120Z[35mDEBUG[0mCREATE TABLE cluster_membership ( membership_partition INTEGER NOT NULL, host_id BYTEA NOT NULL, rpc_address VARCHAR(15) NOT NULL, rpc_port SMALLINT NOT NULL, role SMALLINT NOT NULL, session_start TIMESTAMP DEFAULT '1970-01-01 00:00:01+00:00', last_heartbeat TIMESTAMP DEFAULT '1970-01-01 00:00:01+00:00', record_expiry TIMESTAMP DEFAULT '1970-01-01 00:00:01+00:00', PRIMARY KEY (membership_partition, host_id) );{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.123Z[35mDEBUG[0mCREATE UNIQUE INDEX cm_idx_rolehost ON cluster_membership (role, host_id);{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.124Z[35mDEBUG[0mCREATE INDEX cm_idx_rolelasthb ON cluster_membership (role, last_heartbeat);{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.125Z[35mDEBUG[0mCREATE INDEX cm_idx_rpchost ON cluster_membership (rpc_address, role);{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.126Z[35mDEBUG[0mCREATE INDEX cm_idx_lasthb ON cluster_membership (last_heartbeat);{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.128Z[35mDEBUG[0mCREATE INDEX cm_idx_recordexpiry ON cluster_membership (record_expiry);{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.129Z[35mDEBUG[0m---- Done ----{"logging-call-at": "updatetask.go:169"}
    temporal             | 2026-01-20T03:18:16.130Z[35mDEBUG[0mSchema updated from 0.0 to 1.0{"logging-call-at": "updatetask.go:142"}
    temporal             | 2026-01-20T03:18:16.130Z[35mDEBUG[0m---- Executing updates for version 1.1 ----{"logging-call-at": "updatetask.go:150"}
    temporal             | 2026-01-20T03:18:16.130Z[35mDEBUG[0mALTER TABLE cluster_metadata ADD data BYTEA NOT NULL DEFAULT '';{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.130Z[35mDEBUG[0mALTER TABLE cluster_metadata ADD data_encoding VARCHAR(16) NOT NULL DEFAULT 'Proto3';{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.130Z[35mDEBUG[0mALTER TABLE cluster_metadata ADD version BIGINT NOT NULL DEFAULT 1;{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.131Z[35mDEBUG[0m---- Done ----{"logging-call-at": "updatetask.go:169"}
    temporal             | 2026-01-20T03:18:16.131Z[35mDEBUG[0mSchema updated from 1.0 to 1.1{"logging-call-at": "updatetask.go:142"}
    temporal             | 2026-01-20T03:18:16.131Z[35mDEBUG[0m---- Executing updates for version 1.2 ----{"logging-call-at": "updatetask.go:150"}
    temporal             | 2026-01-20T03:18:16.131Z[35mDEBUG[0mALTER TABLE queue ADD message_encoding VARCHAR(16) NOT NULL DEFAULT 'Json';{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.132Z[35mDEBUG[0mALTER TABLE queue_metadata ADD data_encoding VARCHAR(16) NOT NULL DEFAULT 'Json';{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.132Z[35mDEBUG[0m---- Done ----{"logging-call-at": "updatetask.go:169"}
    temporal             | 2026-01-20T03:18:16.132Z[35mDEBUG[0mSchema updated from 1.1 to 1.2{"logging-call-at": "updatetask.go:142"}
    temporal             | 2026-01-20T03:18:16.132Z[35mDEBUG[0m---- Executing updates for version 1.3 ----{"logging-call-at": "updatetask.go:150"}
    temporal             | 2026-01-20T03:18:16.132Z[35mDEBUG[0mCREATE TABLE visibility_tasks( shard_id INTEGER NOT NULL, task_id BIGINT NOT NULL, data BYTEA NOT NULL, data_encoding VARCHAR(16) NOT NULL, PRIMARY KEY (shard_id, task_id) );{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.135Z[35mDEBUG[0m---- Done ----{"logging-call-at": "updatetask.go:169"}
    temporal             | 2026-01-20T03:18:16.135Z[35mDEBUG[0mSchema updated from 1.2 to 1.3{"logging-call-at": "updatetask.go:142"}
    temporal             | 2026-01-20T03:18:16.135Z[35mDEBUG[0m---- Executing updates for version 1.4 ----{"logging-call-at": "updatetask.go:150"}
    temporal             | 2026-01-20T03:18:16.135Z[35mDEBUG[0mALTER TABLE cluster_metadata DROP immutable_data;{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.136Z[35mDEBUG[0mALTER TABLE cluster_metadata DROP immutable_data_encoding;{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.136Z[35mDEBUG[0m---- Done ----{"logging-call-at": "updatetask.go:169"}
    temporal             | 2026-01-20T03:18:16.137Z[35mDEBUG[0mSchema updated from 1.3 to 1.4{"logging-call-at": "updatetask.go:142"}
    temporal             | 2026-01-20T03:18:16.137Z[35mDEBUG[0m---- Executing updates for version 1.5 ----{"logging-call-at": "updatetask.go:150"}
    temporal             | 2026-01-20T03:18:16.137Z[35mDEBUG[0mALTER TABLE history_node ADD prev_txn_id BIGINT NOT NULL DEFAULT 0;{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.137Z[35mDEBUG[0mALTER TABLE executions ADD db_record_version BIGINT NOT NULL DEFAULT 0;{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.138Z[35mDEBUG[0mALTER TABLE cluster_membership ALTER COLUMN rpc_address TYPE VARCHAR(128);{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.140Z[35mDEBUG[0m---- Done ----{"logging-call-at": "updatetask.go:169"}
    temporal             | 2026-01-20T03:18:16.140Z[35mDEBUG[0mSchema updated from 1.4 to 1.5{"logging-call-at": "updatetask.go:142"}
    temporal             | 2026-01-20T03:18:16.140Z[35mDEBUG[0m---- Executing updates for version 1.6 ----{"logging-call-at": "updatetask.go:150"}
    temporal             | 2026-01-20T03:18:16.140Z[35mDEBUG[0mALTER TABLE queue_metadata ADD version BIGINT NOT NULL DEFAULT 0;{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.141Z[35mDEBUG[0m---- Done ----{"logging-call-at": "updatetask.go:169"}
    temporal             | 2026-01-20T03:18:16.141Z[35mDEBUG[0mSchema updated from 1.5 to 1.6{"logging-call-at": "updatetask.go:142"}
    temporal             | 2026-01-20T03:18:16.141Z[35mDEBUG[0m---- Executing updates for version 1.7 ----{"logging-call-at": "updatetask.go:150"}
    temporal             | 2026-01-20T03:18:16.141Z[35mDEBUG[0mCREATE TABLE cluster_metadata_info ( metadata_partition INTEGER NOT NULL, cluster_name VARCHAR(255) NOT NULL, data BYTEA NOT NULL, data_encoding VARCHAR(16) NOT NULL, version BIGINT NOT NULL, PRIMARY KEY(metadata_partition, cluster_name) );{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.143Z[35mDEBUG[0mALTER TABLE current_executions ALTER COLUMN start_version SET DEFAULT 0;{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.144Z[35mDEBUG[0mCREATE TABLE tiered_storage_tasks( shard_id INTEGER NOT NULL, task_id BIGINT NOT NULL, data BYTEA NOT NULL, data_encoding VARCHAR(16) NOT NULL, PRIMARY KEY (shard_id, task_id) );{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.146Z[35mDEBUG[0m---- Done ----{"logging-call-at": "updatetask.go:169"}
    temporal             | 2026-01-20T03:18:16.146Z[35mDEBUG[0mSchema updated from 1.6 to 1.7{"logging-call-at": "updatetask.go:142"}
    temporal             | 2026-01-20T03:18:16.146Z[35mDEBUG[0m---- Executing updates for version 1.8 ----{"logging-call-at": "updatetask.go:150"}
    temporal             | 2026-01-20T03:18:16.146Z[35mDEBUG[0mDROP TABLE tiered_storage_tasks;{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.147Z[35mDEBUG[0mALTER TABLE current_executions ALTER COLUMN create_request_id TYPE VARCHAR(255);{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.147Z[35mDEBUG[0mALTER TABLE signals_requested_sets ALTER COLUMN signal_id TYPE VARCHAR(255);{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.148Z[35mDEBUG[0m---- Done ----{"logging-call-at": "updatetask.go:169"}
    temporal             | 2026-01-20T03:18:16.149Z[35mDEBUG[0mSchema updated from 1.7 to 1.8{"logging-call-at": "updatetask.go:142"}
    temporal             | 2026-01-20T03:18:16.149Z[35mDEBUG[0m---- Executing updates for version 1.9 ----{"logging-call-at": "updatetask.go:150"}
    temporal             | 2026-01-20T03:18:16.149Z[35mDEBUG[0mCREATE TABLE history_immediate_tasks( shard_id INTEGER NOT NULL, category_id INTEGER NOT NULL, task_id BIGINT NOT NULL, data BYTEA NOT NULL, data_encoding VARCHAR(16) NOT NULL, PRIMARY KEY (shard_id, category_id, task_id) );{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.151Z[35mDEBUG[0mCREATE TABLE history_scheduled_tasks ( shard_id INTEGER NOT NULL, category_id INTEGER NOT NULL, visibility_timestamp TIMESTAMP NOT NULL, task_id BIGINT NOT NULL, data BYTEA NOT NULL, data_encoding VARCHAR(16) NOT NULL, PRIMARY KEY (shard_id, category_id, visibility_timestamp, task_id) );{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.153Z[35mDEBUG[0m---- Done ----{"logging-call-at": "updatetask.go:169"}
    temporal             | 2026-01-20T03:18:16.154Z[35mDEBUG[0mSchema updated from 1.8 to 1.9{"logging-call-at": "updatetask.go:142"}
    temporal             | 2026-01-20T03:18:16.154Z[35mDEBUG[0m---- Executing updates for version 1.10 ----{"logging-call-at": "updatetask.go:150"}
    temporal             | 2026-01-20T03:18:16.154Z[35mDEBUG[0mCREATE TABLE task_queue_user_data ( namespace_id BYTEA NOT NULL, task_queue_name VARCHAR(255) NOT NULL, data BYTEA NOT NULL, data_encoding VARCHAR(16) NOT NULL, version BIGINT NOT NULL, PRIMARY KEY (namespace_id, task_queue_name) );{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.156Z[35mDEBUG[0mCREATE TABLE build_id_to_task_queue ( namespace_id BYTEA NOT NULL, build_id VARCHAR(255) NOT NULL, task_queue_name VARCHAR(255) NOT NULL, PRIMARY KEY (namespace_id, build_id, task_queue_name) );{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.158Z[35mDEBUG[0m---- Done ----{"logging-call-at": "updatetask.go:169"}
    temporal             | 2026-01-20T03:18:16.158Z[35mDEBUG[0mSchema updated from 1.9 to 1.10{"logging-call-at": "updatetask.go:142"}
    temporal             | 2026-01-20T03:18:16.158Z[34mINFO[0mUpdateSchemeTask done{"logging-call-at": "updatetask.go:120"}
    temporal             | 2026-01-20T03:18:16.203Z[34mINFO[0mStarting schema setup{"config": {"SchemaFilePath":"","InitialVersion":"0.0","Overwrite":false,"DisableVersioning":false}, "logging-call-at": "setuptask.go:58"}
    temporal             | 2026-01-20T03:18:16.203Z[35mDEBUG[0mSetting up version tables{"logging-call-at": "setuptask.go:68"}
    temporal             | 2026-01-20T03:18:16.208Z[35mDEBUG[0mSetting initial schema version to 0.0{"logging-call-at": "setuptask.go:119"}
    temporal             | 2026-01-20T03:18:16.208Z[35mDEBUG[0mUpdating schema update log{"logging-call-at": "setuptask.go:124"}
    temporal             | 2026-01-20T03:18:16.209Z[34mINFO[0mSchema setup complete{"logging-call-at": "setuptask.go:132"}
    temporal             | 2026-01-20T03:18:16.219Z[34mINFO[0mUpdateSchemeTask started{"config": {"DBName":"","TargetVersion":"","SchemaDir":"/etc/temporal/schema/postgresql/v96/visibility/versioned","IsDryRun":false}, "logging-call-at": "updatetask.go:97"}
    temporal             | 2026-01-20T03:18:16.220Z[35mDEBUG[0mSchema Dirs: [v1.0 v1.1]{"logging-call-at": "updatetask.go:195"}
    temporal             | 2026-01-20T03:18:16.220Z[34mINFO[0mProcessing schema file: /etc/temporal/schema/postgresql/v96/visibility/versioned/v1.0/schema.sql{"logging-call-at": "updatetask.go:241"}
    temporal             | 2026-01-20T03:18:16.220Z[34mINFO[0mProcessing schema file: /etc/temporal/schema/postgresql/v96/visibility/versioned/v1.1/index.sql{"logging-call-at": "updatetask.go:241"}
    temporal             | 2026-01-20T03:18:16.220Z[35mDEBUG[0mrunning 2 updates for current version 0.0{"logging-call-at": "updatetask.go:131"}
    temporal             | 2026-01-20T03:18:16.221Z[35mDEBUG[0m---- Executing updates for version 1.0 ----{"logging-call-at": "updatetask.go:150"}
    temporal             | 2026-01-20T03:18:16.221Z[35mDEBUG[0mCREATE TABLE executions_visibility ( namespace_id CHAR(64) NOT NULL, run_id CHAR(64) NOT NULL, start_time TIMESTAMP NOT NULL, execution_time TIMESTAMP NOT NULL, workflow_id VARCHAR(255) NOT NULL, workflow_type_name VARCHAR(255) NOT NULL, status INTEGER NOT NULL, close_time TIMESTAMP NULL, history_length BIGINT, memo BYTEA, encoding VARCHAR(64) NOT NULL, task_queue VARCHAR(255) DEFAULT '' NOT NULL, PRIMARY KEY (namespace_id, run_id) );{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.224Z[35mDEBUG[0mCREATE INDEX by_type_start_time ON executions_visibility (namespace_id, workflow_type_name, status, start_time DESC, run_id);{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.225Z[35mDEBUG[0mCREATE INDEX by_workflow_id_start_time ON executions_visibility (namespace_id, workflow_id, status, start_time DESC, run_id);{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.226Z[35mDEBUG[0mCREATE INDEX by_status_by_start_time ON executions_visibility (namespace_id, status, start_time DESC, run_id);{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.226Z[35mDEBUG[0mCREATE INDEX by_type_close_time ON executions_visibility (namespace_id, workflow_type_name, status, close_time DESC, run_id);{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.227Z[35mDEBUG[0mCREATE INDEX by_workflow_id_close_time ON executions_visibility (namespace_id, workflow_id, status, close_time DESC, run_id);{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.229Z[35mDEBUG[0mCREATE INDEX by_status_by_close_time ON executions_visibility (namespace_id, status, close_time DESC, run_id);{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.232Z[35mDEBUG[0m---- Done ----{"logging-call-at": "updatetask.go:169"}
    temporal             | 2026-01-20T03:18:16.232Z[35mDEBUG[0mSchema updated from 0.0 to 1.0{"logging-call-at": "updatetask.go:142"}
    temporal             | 2026-01-20T03:18:16.232Z[35mDEBUG[0m---- Executing updates for version 1.1 ----{"logging-call-at": "updatetask.go:150"}
    temporal             | 2026-01-20T03:18:16.232Z[35mDEBUG[0mCREATE INDEX by_close_time_by_status ON executions_visibility (namespace_id, close_time DESC, run_id, status);{"logging-call-at": "updatetask.go:152"}
    temporal             | 2026-01-20T03:18:16.233Z[35mDEBUG[0m---- Done ----{"logging-call-at": "updatetask.go:169"}
    temporal             | 2026-01-20T03:18:16.234Z[35mDEBUG[0mSchema updated from 1.0 to 1.1{"logging-call-at": "updatetask.go:142"}
    temporal             | 2026-01-20T03:18:16.234Z[34mINFO[0mUpdateSchemeTask done{"logging-call-at": "updatetask.go:120"}
    temporal             | Temporal CLI address: 172.18.0.3:7233.
    temporal             | 2026/01/20 03:18:16 Loading config; env=docker,zone=,configDir=config
    temporal             | 2026/01/20 03:18:16 Loading config files=[config/docker.yaml]
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.261Z","msg":"Build info.","git-time":"2024-01-12T20:57:30.000Z","git-revision":"ca74b18c7257fdb8a87a27afc89827901e152d15","git-modified":true,"go-arch":"arm64","go-os":"linux","go-version":"go1.20.10","cgo-enabled":false,"server-version":"1.22.4","debug-mode":false,"logging-call-at":"main.go:148"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.262Z","msg":"dynamic config changed for the key: system.advancedvisibilitywritingmode oldValue: nil newValue: { constraints: {} value: off }","logging-call-at":"file_based_client.go:275"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.262Z","msg":"dynamic config changed for the key: limit.maxidlength oldValue: nil newValue: { constraints: {} value: 255 }","logging-call-at":"file_based_client.go:275"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.262Z","msg":"dynamic config changed for the key: frontend.enableclientversioncheck oldValue: nil newValue: { constraints: {} value: false }","logging-call-at":"file_based_client.go:275"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.262Z","msg":"dynamic config changed for the key: worker.replicatorconcurrency oldValue: nil newValue: { constraints: {} value: 1000 }","logging-call-at":"file_based_client.go:275"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.262Z","msg":"dynamic config changed for the key: system.defaultworkflowretentionperiod oldValue: nil newValue: { constraints: {} value: 168h }","logging-call-at":"file_based_client.go:275"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.262Z","msg":"dynamic config changed for the key: history.maxautoresetpoints oldValue: nil newValue: { constraints: {} value: 20 }","logging-call-at":"file_based_client.go:275"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.262Z","msg":"dynamic config changed for the key: matching.numtaskqueuereadpartitions oldValue: nil newValue: { constraints: {} value: 4 }","logging-call-at":"file_based_client.go:275"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.262Z","msg":"dynamic config changed for the key: system.enablenamespacenotactiveautoforwarding oldValue: nil newValue: { constraints: {} value: true }","logging-call-at":"file_based_client.go:275"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.262Z","msg":"dynamic config changed for the key: matching.numtaskqueuewritepartitions oldValue: nil newValue: { constraints: {} value: 4 }","logging-call-at":"file_based_client.go:275"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.262Z","msg":"Updated dynamic config","logging-call-at":"file_based_client.go:195"}
    temporal             | {"level":"warn","ts":"2026-01-20T03:18:16.262Z","msg":"Not using any authorizer and flag `--allow-no-auth` not detected. Future versions will require using the flag `--allow-no-auth` if you do not want to set an authorizer.","logging-call-at":"main.go:178"}
    temporal             | Error: unable to health check "temporal.api.workflowservice.v1.WorkflowService" service: connection error: desc = "transport: Error while dialing: dial tcp 172.18.0.3:7233: connect: connection refused"
    temporal             | ('export TEMPORAL_CLI_SHOW_STACKS=1' to see stack traces)
    temporal             | Waiting for Temporal server to start...
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.277Z","msg":"Use rpc address 127.0.0.1:7233 for cluster active.","component":"metadata-initializer","logging-call-at":"fx.go:732"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.286Z","msg":"historyClient: ownership caching disabled","service":"history","logging-call-at":"client.go:82"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.290Z","msg":"Created gRPC listener","service":"history","address":"172.18.0.3:7234","logging-call-at":"rpc.go:152"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.297Z","msg":"Created gRPC listener","service":"matching","address":"172.18.0.3:7235","logging-call-at":"rpc.go:152"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.297Z","msg":"historyClient: ownership caching disabled","service":"matching","logging-call-at":"client.go:82"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.306Z","msg":"historyClient: ownership caching disabled","service":"frontend","logging-call-at":"client.go:82"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.306Z","msg":"Created gRPC listener","service":"frontend","address":"172.18.0.3:7233","logging-call-at":"rpc.go:152"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.309Z","msg":"Service is not requested, skipping initialization.","service":"internal-frontend","logging-call-at":"fx.go:477"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.315Z","msg":"historyClient: ownership caching disabled","service":"worker","logging-call-at":"client.go:82"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.318Z","msg":"PProf not started due to port not set","logging-call-at":"pprof.go:67"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.318Z","msg":"Starting server for services","value":{"frontend":{},"history":{},"matching":{},"worker":{}},"logging-call-at":"server_impl.go:94"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.325Z","msg":"fifo scheduler started","service":"history","component":"memory-scheduled-queue-processor","logging-call-at":"fifo_scheduler.go:96"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.325Z","msg":"fifo scheduler started","service":"history","logging-call-at":"fifo_scheduler.go:96"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.325Z","msg":"interleaved weighted round robin task scheduler started","service":"history","logging-call-at":"interleaved_weighted_round_robin.go:197"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.326Z","msg":"fifo scheduler started","service":"history","logging-call-at":"fifo_scheduler.go:96"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.326Z","msg":"interleaved weighted round robin task scheduler started","service":"history","logging-call-at":"interleaved_weighted_round_robin.go:197"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.326Z","msg":"RuntimeMetricsReporter started","service":"matching","logging-call-at":"runtime.go:138"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.326Z","msg":"RuntimeMetricsReporter started","service":"worker","logging-call-at":"runtime.go:138"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.326Z","msg":"matching starting","service":"matching","logging-call-at":"service.go:91"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.326Z","msg":"worker starting","service":"worker","component":"worker","logging-call-at":"service.go:379"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.327Z","msg":"fifo scheduler started","service":"history","logging-call-at":"fifo_scheduler.go:96"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.327Z","msg":"interleaved weighted round robin task scheduler started","service":"history","logging-call-at":"interleaved_weighted_round_robin.go:197"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.327Z","msg":"fifo scheduler started","service":"history","logging-call-at":"fifo_scheduler.go:96"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.327Z","msg":"interleaved weighted round robin task scheduler started","service":"history","logging-call-at":"interleaved_weighted_round_robin.go:197"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.328Z","msg":"RuntimeMetricsReporter started","service":"frontend","logging-call-at":"runtime.go:138"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.328Z","msg":"frontend starting","service":"frontend","logging-call-at":"service.go:336"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.328Z","msg":"Starting to serve on matching listener","service":"matching","logging-call-at":"service.go:104"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.328Z","msg":"Starting to serve on frontend listener","service":"frontend","logging-call-at":"service.go:355"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.330Z","msg":"RuntimeMetricsReporter started","service":"history","logging-call-at":"runtime.go:138"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.330Z","msg":"Membership heartbeat upserted successfully","address":"172.18.0.3","port":6939,"hostId":"a95eab7e-f5ae-11f0-ae14-826a31043194","logging-call-at":"monitor.go:256"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.330Z","msg":"history starting","service":"history","logging-call-at":"service.go:91"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.330Z","msg":"Replication task fetchers started.","logging-call-at":"task_fetcher.go:142"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.330Z","msg":"none","component":"shard-controller","address":"172.18.0.3:7234","lifecycle":"Started","logging-call-at":"controller_impl.go:136"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.330Z","msg":"Starting to serve on history listener","service":"history","logging-call-at":"service.go:103"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.330Z","msg":"bootstrap hosts fetched","bootstrap-hostports":"172.18.0.3:6939","logging-call-at":"monitor.go:298"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.331Z","msg":"sequential scheduler started","logging-call-at":"sequential_scheduler.go:96"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.331Z","msg":"Current reachable members","component":"service-resolver","service":"worker","addresses":["172.18.0.3:7239"],"logging-call-at":"service_resolver.go:279"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.331Z","msg":"Membership heartbeat upserted successfully","address":"172.18.0.3","port":6935,"hostId":"a95bdbf2-f5ae-11f0-ae14-826a31043194","logging-call-at":"monitor.go:256"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.332Z","msg":"Membership heartbeat upserted successfully","address":"172.18.0.3","port":6934,"hostId":"a95a2d03-f5ae-11f0-ae14-826a31043194","logging-call-at":"monitor.go:256"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.332Z","msg":"bootstrap hosts fetched","bootstrap-hostports":"172.18.0.3:6934,172.18.0.3:6935,172.18.0.3:6939","logging-call-at":"monitor.go:298"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.332Z","msg":"bootstrap hosts fetched","bootstrap-hostports":"172.18.0.3:6935,172.18.0.3:6939,172.18.0.3:6934","logging-call-at":"monitor.go:298"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.333Z","msg":"Current reachable members","component":"service-resolver","service":"matching","addresses":["172.18.0.3:7235"],"logging-call-at":"service_resolver.go:279"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.333Z","msg":"Current reachable members","component":"service-resolver","service":"worker","addresses":["172.18.0.3:7239"],"logging-call-at":"service_resolver.go:279"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.333Z","msg":"Current reachable members","component":"service-resolver","service":"history","addresses":["172.18.0.3:7234"],"logging-call-at":"service_resolver.go:279"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.333Z","msg":"Current reachable members","component":"service-resolver","service":"worker","addresses":["172.18.0.3:7239"],"logging-call-at":"service_resolver.go:279"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.333Z","msg":"none","component":"shard-controller","address":"172.18.0.3:7234","component":"shard-controller","address":"172.18.0.3:7234","shard-update":"RingMembershipChangedEvent","number-processed":1,"number-deleted":0,"logging-call-at":"ownership.go:116"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.333Z","msg":"Current reachable members","component":"service-resolver","service":"matching","addresses":["172.18.0.3:7235"],"logging-call-at":"service_resolver.go:279"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.334Z","msg":"none","shard-id":3,"address":"172.18.0.3:7234","lifecycle":"Started","component":"shard-context","logging-call-at":"context_impl.go:1482"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.334Z","msg":"none","component":"shard-controller","address":"172.18.0.3:7234","numShards":1,"logging-call-at":"controller_impl.go:286"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.334Z","msg":"none","shard-id":1,"address":"172.18.0.3:7234","lifecycle":"Started","component":"shard-context","logging-call-at":"context_impl.go:1482"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.334Z","msg":"none","component":"shard-controller","address":"172.18.0.3:7234","numShards":2,"logging-call-at":"controller_impl.go:286"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.334Z","msg":"none","shard-id":4,"address":"172.18.0.3:7234","lifecycle":"Started","component":"shard-context","logging-call-at":"context_impl.go:1482"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.334Z","msg":"none","component":"shard-controller","address":"172.18.0.3:7234","numShards":3,"logging-call-at":"controller_impl.go:286"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.334Z","msg":"none","shard-id":2,"address":"172.18.0.3:7234","lifecycle":"Started","component":"shard-context","logging-call-at":"context_impl.go:1482"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.334Z","msg":"none","component":"shard-controller","address":"172.18.0.3:7234","numShards":4,"logging-call-at":"controller_impl.go:286"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.334Z","msg":"Current reachable members","component":"service-resolver","service":"history","addresses":["172.18.0.3:7234"],"logging-call-at":"service_resolver.go:279"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.335Z","msg":"Membership heartbeat upserted successfully","address":"172.18.0.3","port":6933,"hostId":"a95d4095-f5ae-11f0-ae14-826a31043194","logging-call-at":"monitor.go:256"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.336Z","msg":"bootstrap hosts fetched","bootstrap-hostports":"172.18.0.3:6933,172.18.0.3:6939,172.18.0.3:6934,172.18.0.3:6935","logging-call-at":"monitor.go:298"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.337Z","msg":"Current reachable members","component":"service-resolver","service":"frontend","addresses":["172.18.0.3:7233"],"logging-call-at":"service_resolver.go:279"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.337Z","msg":"Range updated for shardID","shard-id":3,"address":"172.18.0.3:7234","shard-range-id":1,"previous-shard-range-id":0,"number":0,"next-number":0,"logging-call-at":"context_impl.go:1147"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.337Z","msg":"Acquired shard","shard-id":3,"address":"172.18.0.3:7234","logging-call-at":"context_impl.go:1839"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.337Z","msg":"none","shard-id":3,"address":"172.18.0.3:7234","lifecycle":"Starting","component":"shard-engine","logging-call-at":"context_impl.go:1358"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.337Z","msg":"Current reachable members","component":"service-resolver","service":"matching","addresses":["172.18.0.3:7235"],"logging-call-at":"service_resolver.go:279"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.337Z","msg":"Current reachable members","component":"service-resolver","service":"worker","addresses":["172.18.0.3:7239"],"logging-call-at":"service_resolver.go:279"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.337Z","msg":"Frontend is now healthy","service":"frontend","logging-call-at":"workflow_handler.go:219"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.337Z","msg":"none","shard-id":3,"address":"172.18.0.3:7234","component":"history-engine","lifecycle":"Starting","logging-call-at":"history_engine.go:286"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.337Z","msg":"none","shard-id":3,"address":"172.18.0.3:7234","component":"visibility-queue-processor","lifecycle":"Starting","logging-call-at":"queue_immediate.go:114"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.337Z","msg":"Task rescheduler started.","shard-id":3,"address":"172.18.0.3:7234","component":"visibility-queue-processor","lifecycle":"Started","logging-call-at":"rescheduler.go:124"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.337Z","msg":"none","shard-id":3,"address":"172.18.0.3:7234","component":"visibility-queue-processor","lifecycle":"Started","logging-call-at":"queue_immediate.go:123"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.337Z","msg":"none","shard-id":3,"address":"172.18.0.3:7234","component":"archival-queue-processor","lifecycle":"Starting","logging-call-at":"queue_scheduled.go:154"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.337Z","msg":"Task rescheduler started.","shard-id":3,"address":"172.18.0.3:7234","component":"archival-queue-processor","lifecycle":"Started","logging-call-at":"rescheduler.go:124"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.337Z","msg":"none","shard-id":3,"address":"172.18.0.3:7234","component":"archival-queue-processor","lifecycle":"Started","logging-call-at":"queue_scheduled.go:163"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.337Z","msg":"none","shard-id":3,"address":"172.18.0.3:7234","component":"timer-queue-processor","lifecycle":"Starting","logging-call-at":"queue_scheduled.go:154"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.337Z","msg":"Task rescheduler started.","shard-id":3,"address":"172.18.0.3:7234","component":"timer-queue-processor","lifecycle":"Started","logging-call-at":"rescheduler.go:124"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.337Z","msg":"none","shard-id":3,"address":"172.18.0.3:7234","component":"timer-queue-processor","lifecycle":"Started","logging-call-at":"queue_scheduled.go:163"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.337Z","msg":"none","shard-id":3,"address":"172.18.0.3:7234","component":"transfer-queue-processor","lifecycle":"Starting","logging-call-at":"queue_immediate.go:114"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.337Z","msg":"Task rescheduler started.","shard-id":3,"address":"172.18.0.3:7234","component":"transfer-queue-processor","lifecycle":"Started","logging-call-at":"rescheduler.go:124"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.337Z","msg":"none","shard-id":3,"address":"172.18.0.3:7234","component":"transfer-queue-processor","lifecycle":"Started","logging-call-at":"queue_immediate.go:123"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.337Z","msg":"none","service":"history","component":"memory-scheduled-queue-processor","lifecycle":"Starting","logging-call-at":"memory_scheduled_queue.go:103"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.337Z","msg":"none","service":"history","component":"memory-scheduled-queue-processor","lifecycle":"Started","logging-call-at":"memory_scheduled_queue.go:108"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.337Z","msg":"none","shard-id":3,"address":"172.18.0.3:7234","component":"history-engine","lifecycle":"Started","logging-call-at":"history_engine.go:295"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.337Z","msg":"none","shard-id":3,"address":"172.18.0.3:7234","lifecycle":"Started","component":"shard-engine","logging-call-at":"context_impl.go:1361"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.337Z","msg":"queue reader started","shard-id":3,"address":"172.18.0.3:7234","component":"visibility-queue-processor","queue-reader-id":0,"lifecycle":"Started","logging-call-at":"reader.go:182"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.338Z","msg":"queue reader started","shard-id":3,"address":"172.18.0.3:7234","component":"timer-queue-processor","queue-reader-id":0,"lifecycle":"Started","logging-call-at":"reader.go:182"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.338Z","msg":"queue reader started","shard-id":3,"address":"172.18.0.3:7234","component":"transfer-queue-processor","queue-reader-id":0,"lifecycle":"Started","logging-call-at":"reader.go:182"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.339Z","msg":"queue reader started","shard-id":3,"address":"172.18.0.3:7234","component":"archival-queue-processor","queue-reader-id":0,"lifecycle":"Started","logging-call-at":"reader.go:182"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.341Z","msg":"Range updated for shardID","shard-id":2,"address":"172.18.0.3:7234","shard-range-id":1,"previous-shard-range-id":0,"number":0,"next-number":0,"logging-call-at":"context_impl.go:1147"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.341Z","msg":"Acquired shard","shard-id":2,"address":"172.18.0.3:7234","logging-call-at":"context_impl.go:1839"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.341Z","msg":"none","shard-id":2,"address":"172.18.0.3:7234","lifecycle":"Starting","component":"shard-engine","logging-call-at":"context_impl.go:1358"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"Current reachable members","component":"service-resolver","service":"frontend","addresses":["172.18.0.3:7233"],"logging-call-at":"service_resolver.go:279"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"none","shard-id":2,"address":"172.18.0.3:7234","component":"history-engine","lifecycle":"Starting","logging-call-at":"history_engine.go:286"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"none","shard-id":2,"address":"172.18.0.3:7234","component":"transfer-queue-processor","lifecycle":"Starting","logging-call-at":"queue_immediate.go:114"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"Task rescheduler started.","shard-id":2,"address":"172.18.0.3:7234","component":"transfer-queue-processor","lifecycle":"Started","logging-call-at":"rescheduler.go:124"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"none","shard-id":2,"address":"172.18.0.3:7234","component":"transfer-queue-processor","lifecycle":"Started","logging-call-at":"queue_immediate.go:123"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"none","service":"history","component":"memory-scheduled-queue-processor","lifecycle":"Starting","logging-call-at":"memory_scheduled_queue.go:103"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"none","service":"history","component":"memory-scheduled-queue-processor","lifecycle":"Started","logging-call-at":"memory_scheduled_queue.go:108"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"none","shard-id":2,"address":"172.18.0.3:7234","component":"visibility-queue-processor","lifecycle":"Starting","logging-call-at":"queue_immediate.go:114"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"Task rescheduler started.","shard-id":2,"address":"172.18.0.3:7234","component":"visibility-queue-processor","lifecycle":"Started","logging-call-at":"rescheduler.go:124"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"none","shard-id":2,"address":"172.18.0.3:7234","component":"visibility-queue-processor","lifecycle":"Started","logging-call-at":"queue_immediate.go:123"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"none","shard-id":2,"address":"172.18.0.3:7234","component":"archival-queue-processor","lifecycle":"Starting","logging-call-at":"queue_scheduled.go:154"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"Task rescheduler started.","shard-id":2,"address":"172.18.0.3:7234","component":"archival-queue-processor","lifecycle":"Started","logging-call-at":"rescheduler.go:124"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"none","shard-id":2,"address":"172.18.0.3:7234","component":"archival-queue-processor","lifecycle":"Started","logging-call-at":"queue_scheduled.go:163"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"none","shard-id":2,"address":"172.18.0.3:7234","component":"timer-queue-processor","lifecycle":"Starting","logging-call-at":"queue_scheduled.go:154"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"Task rescheduler started.","shard-id":2,"address":"172.18.0.3:7234","component":"timer-queue-processor","lifecycle":"Started","logging-call-at":"rescheduler.go:124"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"none","shard-id":2,"address":"172.18.0.3:7234","component":"timer-queue-processor","lifecycle":"Started","logging-call-at":"queue_scheduled.go:163"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"none","shard-id":2,"address":"172.18.0.3:7234","component":"history-engine","lifecycle":"Started","logging-call-at":"history_engine.go:295"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"none","shard-id":2,"address":"172.18.0.3:7234","lifecycle":"Started","component":"shard-engine","logging-call-at":"context_impl.go:1361"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"Range updated for shardID","shard-id":4,"address":"172.18.0.3:7234","shard-range-id":1,"previous-shard-range-id":0,"number":0,"next-number":0,"logging-call-at":"context_impl.go:1147"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"Acquired shard","shard-id":4,"address":"172.18.0.3:7234","logging-call-at":"context_impl.go:1839"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"none","shard-id":4,"address":"172.18.0.3:7234","lifecycle":"Starting","component":"shard-engine","logging-call-at":"context_impl.go:1358"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"none","shard-id":4,"address":"172.18.0.3:7234","component":"history-engine","lifecycle":"Starting","logging-call-at":"history_engine.go:286"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"none","shard-id":4,"address":"172.18.0.3:7234","component":"visibility-queue-processor","lifecycle":"Starting","logging-call-at":"queue_immediate.go:114"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"Task rescheduler started.","shard-id":4,"address":"172.18.0.3:7234","component":"visibility-queue-processor","lifecycle":"Started","logging-call-at":"rescheduler.go:124"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"none","shard-id":4,"address":"172.18.0.3:7234","component":"visibility-queue-processor","lifecycle":"Started","logging-call-at":"queue_immediate.go:123"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"none","shard-id":4,"address":"172.18.0.3:7234","component":"archival-queue-processor","lifecycle":"Starting","logging-call-at":"queue_scheduled.go:154"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"Task rescheduler started.","shard-id":4,"address":"172.18.0.3:7234","component":"archival-queue-processor","lifecycle":"Started","logging-call-at":"rescheduler.go:124"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"none","shard-id":4,"address":"172.18.0.3:7234","component":"archival-queue-processor","lifecycle":"Started","logging-call-at":"queue_scheduled.go:163"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"none","shard-id":4,"address":"172.18.0.3:7234","component":"timer-queue-processor","lifecycle":"Starting","logging-call-at":"queue_scheduled.go:154"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"Task rescheduler started.","shard-id":4,"address":"172.18.0.3:7234","component":"timer-queue-processor","lifecycle":"Started","logging-call-at":"rescheduler.go:124"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"none","shard-id":4,"address":"172.18.0.3:7234","component":"timer-queue-processor","lifecycle":"Started","logging-call-at":"queue_scheduled.go:163"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"none","shard-id":4,"address":"172.18.0.3:7234","component":"transfer-queue-processor","lifecycle":"Starting","logging-call-at":"queue_immediate.go:114"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"Task rescheduler started.","shard-id":4,"address":"172.18.0.3:7234","component":"transfer-queue-processor","lifecycle":"Started","logging-call-at":"rescheduler.go:124"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"none","shard-id":4,"address":"172.18.0.3:7234","component":"transfer-queue-processor","lifecycle":"Started","logging-call-at":"queue_immediate.go:123"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"none","service":"history","component":"memory-scheduled-queue-processor","lifecycle":"Starting","logging-call-at":"memory_scheduled_queue.go:103"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"none","service":"history","component":"memory-scheduled-queue-processor","lifecycle":"Started","logging-call-at":"memory_scheduled_queue.go:108"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"none","shard-id":4,"address":"172.18.0.3:7234","component":"history-engine","lifecycle":"Started","logging-call-at":"history_engine.go:295"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"none","shard-id":4,"address":"172.18.0.3:7234","lifecycle":"Started","component":"shard-engine","logging-call-at":"context_impl.go:1361"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"queue reader started","shard-id":4,"address":"172.18.0.3:7234","component":"visibility-queue-processor","queue-reader-id":0,"lifecycle":"Started","logging-call-at":"reader.go:182"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"queue reader started","shard-id":2,"address":"172.18.0.3:7234","component":"transfer-queue-processor","queue-reader-id":0,"lifecycle":"Started","logging-call-at":"reader.go:182"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"queue reader started","shard-id":4,"address":"172.18.0.3:7234","component":"archival-queue-processor","queue-reader-id":0,"lifecycle":"Started","logging-call-at":"reader.go:182"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"queue reader started","shard-id":2,"address":"172.18.0.3:7234","component":"archival-queue-processor","queue-reader-id":0,"lifecycle":"Started","logging-call-at":"reader.go:182"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"queue reader started","shard-id":2,"address":"172.18.0.3:7234","component":"timer-queue-processor","queue-reader-id":0,"lifecycle":"Started","logging-call-at":"reader.go:182"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"queue reader started","shard-id":2,"address":"172.18.0.3:7234","component":"visibility-queue-processor","queue-reader-id":0,"lifecycle":"Started","logging-call-at":"reader.go:182"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"queue reader started","shard-id":4,"address":"172.18.0.3:7234","component":"transfer-queue-processor","queue-reader-id":0,"lifecycle":"Started","logging-call-at":"reader.go:182"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.343Z","msg":"queue reader started","shard-id":4,"address":"172.18.0.3:7234","component":"timer-queue-processor","queue-reader-id":0,"lifecycle":"Started","logging-call-at":"reader.go:182"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.344Z","msg":"Range updated for shardID","shard-id":1,"address":"172.18.0.3:7234","shard-range-id":1,"previous-shard-range-id":0,"number":0,"next-number":0,"logging-call-at":"context_impl.go:1147"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.344Z","msg":"Acquired shard","shard-id":1,"address":"172.18.0.3:7234","logging-call-at":"context_impl.go:1839"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.344Z","msg":"none","shard-id":1,"address":"172.18.0.3:7234","lifecycle":"Starting","component":"shard-engine","logging-call-at":"context_impl.go:1358"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.344Z","msg":"none","shard-id":1,"address":"172.18.0.3:7234","component":"history-engine","lifecycle":"Starting","logging-call-at":"history_engine.go:286"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.344Z","msg":"none","shard-id":1,"address":"172.18.0.3:7234","component":"transfer-queue-processor","lifecycle":"Starting","logging-call-at":"queue_immediate.go:114"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.344Z","msg":"Task rescheduler started.","shard-id":1,"address":"172.18.0.3:7234","component":"transfer-queue-processor","lifecycle":"Started","logging-call-at":"rescheduler.go:124"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.344Z","msg":"none","shard-id":1,"address":"172.18.0.3:7234","component":"transfer-queue-processor","lifecycle":"Started","logging-call-at":"queue_immediate.go:123"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.344Z","msg":"none","service":"history","component":"memory-scheduled-queue-processor","lifecycle":"Starting","logging-call-at":"memory_scheduled_queue.go:103"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.344Z","msg":"none","service":"history","component":"memory-scheduled-queue-processor","lifecycle":"Started","logging-call-at":"memory_scheduled_queue.go:108"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.344Z","msg":"none","shard-id":1,"address":"172.18.0.3:7234","component":"visibility-queue-processor","lifecycle":"Starting","logging-call-at":"queue_immediate.go:114"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.344Z","msg":"Task rescheduler started.","shard-id":1,"address":"172.18.0.3:7234","component":"visibility-queue-processor","lifecycle":"Started","logging-call-at":"rescheduler.go:124"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.344Z","msg":"none","shard-id":1,"address":"172.18.0.3:7234","component":"visibility-queue-processor","lifecycle":"Started","logging-call-at":"queue_immediate.go:123"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.344Z","msg":"none","shard-id":1,"address":"172.18.0.3:7234","component":"archival-queue-processor","lifecycle":"Starting","logging-call-at":"queue_scheduled.go:154"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.344Z","msg":"Task rescheduler started.","shard-id":1,"address":"172.18.0.3:7234","component":"archival-queue-processor","lifecycle":"Started","logging-call-at":"rescheduler.go:124"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.344Z","msg":"none","shard-id":1,"address":"172.18.0.3:7234","component":"archival-queue-processor","lifecycle":"Started","logging-call-at":"queue_scheduled.go:163"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.344Z","msg":"none","shard-id":1,"address":"172.18.0.3:7234","component":"timer-queue-processor","lifecycle":"Starting","logging-call-at":"queue_scheduled.go:154"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.344Z","msg":"Task rescheduler started.","shard-id":1,"address":"172.18.0.3:7234","component":"timer-queue-processor","lifecycle":"Started","logging-call-at":"rescheduler.go:124"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.344Z","msg":"none","shard-id":1,"address":"172.18.0.3:7234","component":"timer-queue-processor","lifecycle":"Started","logging-call-at":"queue_scheduled.go:163"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.344Z","msg":"none","shard-id":1,"address":"172.18.0.3:7234","component":"history-engine","lifecycle":"Started","logging-call-at":"history_engine.go:295"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.344Z","msg":"none","shard-id":1,"address":"172.18.0.3:7234","lifecycle":"Started","component":"shard-engine","logging-call-at":"context_impl.go:1361"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.344Z","msg":"queue reader started","shard-id":1,"address":"172.18.0.3:7234","component":"transfer-queue-processor","queue-reader-id":0,"lifecycle":"Started","logging-call-at":"reader.go:182"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.345Z","msg":"queue reader started","shard-id":1,"address":"172.18.0.3:7234","component":"archival-queue-processor","queue-reader-id":0,"lifecycle":"Started","logging-call-at":"reader.go:182"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.345Z","msg":"queue reader started","shard-id":1,"address":"172.18.0.3:7234","component":"visibility-queue-processor","queue-reader-id":0,"lifecycle":"Started","logging-call-at":"reader.go:182"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.345Z","msg":"queue reader started","shard-id":1,"address":"172.18.0.3:7234","component":"timer-queue-processor","queue-reader-id":0,"lifecycle":"Started","logging-call-at":"reader.go:182"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.361Z","msg":"Current reachable members","component":"service-resolver","service":"frontend","addresses":["172.18.0.3:7233"],"logging-call-at":"service_resolver.go:279"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.365Z","msg":"history client encountered error","service":"frontend","error":"Not enough hosts to serve the request","service-error-type":"serviceerror.Unavailable","logging-call-at":"metric_client.go:104"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.365Z","msg":"history client encountered error","service":"frontend","error":"Not enough hosts to serve the request","service-error-type":"serviceerror.Unavailable","logging-call-at":"metric_client.go:104"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.381Z","msg":"Current reachable members","component":"service-resolver","service":"matching","addresses":["172.18.0.3:7235"],"logging-call-at":"service_resolver.go:279"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.382Z","msg":"Current reachable members","component":"service-resolver","service":"frontend","addresses":["172.18.0.3:7233"],"logging-call-at":"service_resolver.go:279"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.412Z","msg":"history client encountered error","service":"frontend","error":"Not enough hosts to serve the request","service-error-type":"serviceerror.Unavailable","logging-call-at":"metric_client.go:104"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.412Z","msg":"history client encountered error","service":"frontend","error":"Not enough hosts to serve the request","service-error-type":"serviceerror.Unavailable","logging-call-at":"metric_client.go:104"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.515Z","msg":"Started Worker","service":"worker","Namespace":"temporal-system","TaskQueue":"temporal-sys-tq-scanner-taskqueue-0","WorkerID":"1@8b4da51fa81b@","logging-call-at":"scanner.go:239"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.515Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"8b4da51fa81b:96a6134d-bc1f-4502-94e0-e2884361eb7d","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.515Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"temporal-sys-tq-scanner-taskqueue-0","wf-task-queue-type":"Activity","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.516Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"temporal-sys-tq-scanner-taskqueue-0","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.516Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-sys-tq-scanner-taskqueue-0/2","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.516Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"8b4da51fa81b:d25d1059-6173-4e46-9f4e-57fde19c9653","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.516Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-sys-tq-scanner-taskqueue-0/2","wf-task-queue-type":"Activity","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.517Z","msg":"Started Worker","service":"worker","Namespace":"temporal-system","TaskQueue":"temporal-sys-history-scanner-taskqueue-0","WorkerID":"1@8b4da51fa81b@","logging-call-at":"scanner.go:239"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.517Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"temporal-sys-history-scanner-taskqueue-0","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.517Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-sys-history-scanner-taskqueue-0/2","wf-task-queue-type":"Activity","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.517Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-sys-history-scanner-taskqueue-0/3","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.518Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-sys-tq-scanner-taskqueue-0/3","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.518Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-sys-history-scanner-taskqueue-0/1","wf-task-queue-type":"Activity","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.520Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"8b4da51fa81b:519a5d72-d923-4c67-9534-c70c73edb161","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.520Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"temporal-archival-tq","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.521Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"temporal-sys-history-scanner-taskqueue-0","wf-task-queue-type":"Activity","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.522Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-sys-tq-scanner-taskqueue-0/3","wf-task-queue-type":"Activity","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.522Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-sys-tq-scanner-taskqueue-0/1","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.522Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-sys-history-scanner-taskqueue-0/2","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.522Z","msg":"Started Worker","service":"worker","Namespace":"temporal-system","TaskQueue":"temporal-archival-tq","WorkerID":"1@8b4da51fa81b@","logging-call-at":"client_worker.go:137"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.523Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-sys-tq-scanner-taskqueue-0/1","wf-task-queue-type":"Activity","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.529Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-archival-tq/1","wf-task-queue-type":"Activity","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.529Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-archival-tq/3","wf-task-queue-type":"Activity","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.529Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-archival-tq/2","wf-task-queue-type":"Activity","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.529Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-sys-history-scanner-taskqueue-0/1","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.531Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"8b4da51fa81b:7037727b-b0f8-43ef-ab8a-23d6fe618ec1","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.531Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"temporal-sys-processor-parent-close-policy","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.532Z","msg":"Started Worker","service":"worker","Namespace":"temporal-system","TaskQueue":"temporal-sys-processor-parent-close-policy","WorkerID":"1@8b4da51fa81b@","logging-call-at":"processor.go:98"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.534Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-sys-processor-parent-close-policy/3","wf-task-queue-type":"Activity","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.534Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"8b4da51fa81b:1037e04b-50c3-4ce7-86aa-f34900a8c011","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.534Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-sys-processor-parent-close-policy/1","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.535Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"temporal-sys-batcher-taskqueue","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.535Z","msg":"Started Worker","service":"worker","Namespace":"temporal-system","TaskQueue":"temporal-sys-batcher-taskqueue","WorkerID":"1@8b4da51fa81b@","logging-call-at":"batcher.go:98"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.535Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-sys-processor-parent-close-policy/2","wf-task-queue-type":"Activity","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.535Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-sys-batcher-taskqueue/1","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.535Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"temporal-sys-batcher-taskqueue","wf-task-queue-type":"Activity","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.535Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"temporal-sys-processor-parent-close-policy","wf-task-queue-type":"Activity","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.536Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-sys-processor-parent-close-policy/1","wf-task-queue-type":"Activity","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.536Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"8b4da51fa81b:9fdb6b43-faa2-42e4-88af-b0954a803f61","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.536Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"default-worker-tq","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.536Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"temporal-archival-tq","wf-task-queue-type":"Activity","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.537Z","msg":"Started Worker","service":"worker","Namespace":"temporal-system","TaskQueue":"default-worker-tq","WorkerID":"1@8b4da51fa81b@","logging-call-at":"worker.go:101"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.537Z","msg":"none","component":"worker-manager","lifecycle":"Started","logging-call-at":"worker.go:106"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.537Z","msg":"none","component":"perns-worker-manager","lifecycle":"Starting","logging-call-at":"pernamespaceworker.go:166"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.537Z","msg":"none","component":"perns-worker-manager","lifecycle":"Started","logging-call-at":"pernamespaceworker.go:177"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.537Z","msg":"worker service started","service":"worker","component":"worker","address":"172.18.0.3:7239","logging-call-at":"service.go:418"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.537Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/default-worker-tq/3","wf-task-queue-type":"Activity","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.537Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/default-worker-tq/2","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.538Z","msg":"Started Worker","service":"worker","Namespace":"temporal-system","TaskQueue":"temporal-sys-per-ns-tq","WorkerID":"server-worker@1@8b4da51fa81b@temporal-system","logging-call-at":"pernamespaceworker.go:483"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.538Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/default-worker-tq/2","wf-task-queue-type":"Activity","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.538Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"8b4da51fa81b:2bcf5eb0-af4a-44ab-acc7-03fe6d2bbc66","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.538Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"temporal-sys-per-ns-tq","wf-task-queue-type":"Activity","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.539Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"temporal-sys-per-ns-tq","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.539Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"default-worker-tq","wf-task-queue-type":"Activity","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.577Z","msg":"history client encountered error","service":"frontend","error":"Not enough hosts to serve the request","service-error-type":"serviceerror.Unavailable","logging-call-at":"metric_client.go:104"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.608Z","msg":"history client encountered error","service":"frontend","error":"Not enough hosts to serve the request","service-error-type":"serviceerror.Unavailable","logging-call-at":"metric_client.go:104"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.620Z","msg":"history client encountered error","service":"frontend","error":"Not enough hosts to serve the request","service-error-type":"serviceerror.Unavailable","logging-call-at":"metric_client.go:104"}
    temporal             | {"level":"error","ts":"2026-01-20T03:18:16.620Z","msg":"service failures","operation":"StartWorkflowExecution","wf-namespace":"temporal-system","error":"Not enough hosts to serve the request","logging-call-at":"telemetry.go:341","stacktrace":"go.temporal.io/server/common/log.(*zapLogger).Error\n\t/home/builder/temporal/common/log/zap_logger.go:156\ngo.temporal.io/server/common/rpc/interceptor.(*TelemetryInterceptor).handleError\n\t/home/builder/temporal/common/rpc/interceptor/telemetry.go:341\ngo.temporal.io/server/common/rpc/interceptor.(*TelemetryInterceptor).UnaryIntercept\n\t/home/builder/temporal/common/rpc/interceptor/telemetry.go:174\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.temporal.io/server/service/frontend.(*RedirectionInterceptor).handleRedirectAPIInvocation.func2\n\t/home/builder/temporal/service/frontend/redirection_interceptor.go:238\ngo.temporal.io/server/service/frontend.(*NoopRedirectionPolicy).WithNamespaceRedirect\n\t/home/builder/temporal/service/frontend/dc_redirection_policy.go:125\ngo.temporal.io/server/service/frontend.(*RedirectionInterceptor).handleRedirectAPIInvocation\n\t/home/builder/temporal/service/frontend/redirection_interceptor.go:235\ngo.temporal.io/server/service/frontend.(*RedirectionInterceptor).Intercept\n\t/home/builder/temporal/service/frontend/redirection_interceptor.go:195\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.temporal.io/server/common/authorization.(*interceptor).Interceptor\n\t/home/builder/temporal/common/authorization/interceptor.go:158\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.temporal.io/server/common/metrics.NewServerMetricsContextInjectorInterceptor.func1\n\t/home/builder/temporal/common/metrics/grpc.go:66\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc.UnaryServerInterceptor.func1\n\t/go/pkg/mod/go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc@v0.42.0/interceptor.go:344\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.temporal.io/server/common/rpc/interceptor.(*NamespaceLogInterceptor).Intercept\n\t/home/builder/temporal/common/rpc/interceptor/namespace_logger.go:84\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.temporal.io/server/common/rpc/interceptor.(*NamespaceValidatorInterceptor).NamespaceValidateIntercept\n\t/home/builder/temporal/common/rpc/interceptor/namespace_validator.go:111\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.temporal.io/server/common/rpc.ServiceErrorInterceptor\n\t/home/builder/temporal/common/rpc/grpc.go:145\ngoogle.golang.org/grpc.chainUnaryInterceptors.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1186\ngo.temporal.io/api/workflowservice/v1._WorkflowService_StartWorkflowExecution_Handler\n\t/go/pkg/mod/go.temporal.io/api@v1.24.1-0.20231003165936-bb03061759c8/workflowservice/v1/service.pb.go:1627\ngoogle.golang.org/grpc.(*Server).processUnaryRPC\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1376\ngoogle.golang.org/grpc.(*Server).handleStream\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1753\ngoogle.golang.org/grpc.(*Server).serveStreams.func1.1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:998"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:16.660Z","msg":"history client encountered error","service":"frontend","error":"Not enough hosts to serve the request","service-error-type":"serviceerror.Unavailable","logging-call-at":"metric_client.go:104"}
    temporal             | {"level":"error","ts":"2026-01-20T03:18:16.660Z","msg":"service failures","operation":"StartWorkflowExecution","wf-namespace":"temporal-system","error":"Not enough hosts to serve the request","logging-call-at":"telemetry.go:341","stacktrace":"go.temporal.io/server/common/log.(*zapLogger).Error\n\t/home/builder/temporal/common/log/zap_logger.go:156\ngo.temporal.io/server/common/rpc/interceptor.(*TelemetryInterceptor).handleError\n\t/home/builder/temporal/common/rpc/interceptor/telemetry.go:341\ngo.temporal.io/server/common/rpc/interceptor.(*TelemetryInterceptor).UnaryIntercept\n\t/home/builder/temporal/common/rpc/interceptor/telemetry.go:174\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.temporal.io/server/service/frontend.(*RedirectionInterceptor).handleRedirectAPIInvocation.func2\n\t/home/builder/temporal/service/frontend/redirection_interceptor.go:238\ngo.temporal.io/server/service/frontend.(*NoopRedirectionPolicy).WithNamespaceRedirect\n\t/home/builder/temporal/service/frontend/dc_redirection_policy.go:125\ngo.temporal.io/server/service/frontend.(*RedirectionInterceptor).handleRedirectAPIInvocation\n\t/home/builder/temporal/service/frontend/redirection_interceptor.go:235\ngo.temporal.io/server/service/frontend.(*RedirectionInterceptor).Intercept\n\t/home/builder/temporal/service/frontend/redirection_interceptor.go:195\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.temporal.io/server/common/authorization.(*interceptor).Interceptor\n\t/home/builder/temporal/common/authorization/interceptor.go:158\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.temporal.io/server/common/metrics.NewServerMetricsContextInjectorInterceptor.func1\n\t/home/builder/temporal/common/metrics/grpc.go:66\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc.UnaryServerInterceptor.func1\n\t/go/pkg/mod/go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc@v0.42.0/interceptor.go:344\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.temporal.io/server/common/rpc/interceptor.(*NamespaceLogInterceptor).Intercept\n\t/home/builder/temporal/common/rpc/interceptor/namespace_logger.go:84\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.temporal.io/server/common/rpc/interceptor.(*NamespaceValidatorInterceptor).NamespaceValidateIntercept\n\t/home/builder/temporal/common/rpc/interceptor/namespace_validator.go:111\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.temporal.io/server/common/rpc.ServiceErrorInterceptor\n\t/home/builder/temporal/common/rpc/grpc.go:145\ngoogle.golang.org/grpc.chainUnaryInterceptors.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1186\ngo.temporal.io/api/workflowservice/v1._WorkflowService_StartWorkflowExecution_Handler\n\t/go/pkg/mod/go.temporal.io/api@v1.24.1-0.20231003165936-bb03061759c8/workflowservice/v1/service.pb.go:1627\ngoogle.golang.org/grpc.(*Server).processUnaryRPC\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1376\ngoogle.golang.org/grpc.(*Server).handleStream\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1753\ngoogle.golang.org/grpc.(*Server).serveStreams.func1.1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:998"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:17.001Z","msg":"history client encountered error","service":"frontend","error":"Not enough hosts to serve the request","service-error-type":"serviceerror.Unavailable","logging-call-at":"metric_client.go:104"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:17.052Z","msg":"history client encountered error","service":"frontend","error":"Not enough hosts to serve the request","service-error-type":"serviceerror.Unavailable","logging-call-at":"metric_client.go:104"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:17.067Z","msg":"history client encountered error","service":"frontend","error":"Not enough hosts to serve the request","service-error-type":"serviceerror.Unavailable","logging-call-at":"metric_client.go:104"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:17.120Z","msg":"history client encountered error","service":"frontend","error":"Not enough hosts to serve the request","service-error-type":"serviceerror.Unavailable","logging-call-at":"metric_client.go:104"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:17.219Z","msg":"history client encountered error","service":"frontend","error":"Not enough hosts to serve the request","service-error-type":"serviceerror.Unavailable","logging-call-at":"metric_client.go:104"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:17.263Z","msg":"history client encountered error","service":"frontend","error":"Not enough hosts to serve the request","service-error-type":"serviceerror.Unavailable","logging-call-at":"metric_client.go:104"}
    temporal             | {"level":"error","ts":"2026-01-20T03:18:17.263Z","msg":"service failures","operation":"StartWorkflowExecution","wf-namespace":"temporal-system","error":"Not enough hosts to serve the request","logging-call-at":"telemetry.go:341","stacktrace":"go.temporal.io/server/common/log.(*zapLogger).Error\n\t/home/builder/temporal/common/log/zap_logger.go:156\ngo.temporal.io/server/common/rpc/interceptor.(*TelemetryInterceptor).handleError\n\t/home/builder/temporal/common/rpc/interceptor/telemetry.go:341\ngo.temporal.io/server/common/rpc/interceptor.(*TelemetryInterceptor).UnaryIntercept\n\t/home/builder/temporal/common/rpc/interceptor/telemetry.go:174\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.temporal.io/server/service/frontend.(*RedirectionInterceptor).handleRedirectAPIInvocation.func2\n\t/home/builder/temporal/service/frontend/redirection_interceptor.go:238\ngo.temporal.io/server/service/frontend.(*NoopRedirectionPolicy).WithNamespaceRedirect\n\t/home/builder/temporal/service/frontend/dc_redirection_policy.go:125\ngo.temporal.io/server/service/frontend.(*RedirectionInterceptor).handleRedirectAPIInvocation\n\t/home/builder/temporal/service/frontend/redirection_interceptor.go:235\ngo.temporal.io/server/service/frontend.(*RedirectionInterceptor).Intercept\n\t/home/builder/temporal/service/frontend/redirection_interceptor.go:195\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.temporal.io/server/common/authorization.(*interceptor).Interceptor\n\t/home/builder/temporal/common/authorization/interceptor.go:158\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.temporal.io/server/common/metrics.NewServerMetricsContextInjectorInterceptor.func1\n\t/home/builder/temporal/common/metrics/grpc.go:66\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc.UnaryServerInterceptor.func1\n\t/go/pkg/mod/go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc@v0.42.0/interceptor.go:344\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.temporal.io/server/common/rpc/interceptor.(*NamespaceLogInterceptor).Intercept\n\t/home/builder/temporal/common/rpc/interceptor/namespace_logger.go:84\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.temporal.io/server/common/rpc/interceptor.(*NamespaceValidatorInterceptor).NamespaceValidateIntercept\n\t/home/builder/temporal/common/rpc/interceptor/namespace_validator.go:111\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.temporal.io/server/common/rpc.ServiceErrorInterceptor\n\t/home/builder/temporal/common/rpc/grpc.go:145\ngoogle.golang.org/grpc.chainUnaryInterceptors.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1186\ngo.temporal.io/api/workflowservice/v1._WorkflowService_StartWorkflowExecution_Handler\n\t/go/pkg/mod/go.temporal.io/api@v1.24.1-0.20231003165936-bb03061759c8/workflowservice/v1/service.pb.go:1627\ngoogle.golang.org/grpc.(*Server).processUnaryRPC\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1376\ngoogle.golang.org/grpc.(*Server).handleStream\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1753\ngoogle.golang.org/grpc.(*Server).serveStreams.func1.1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:998"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:17.285Z","msg":"history client encountered error","service":"frontend","error":"Not enough hosts to serve the request","service-error-type":"serviceerror.Unavailable","logging-call-at":"metric_client.go:104"}
    temporal             | Temporal server started.
    temporal             | Registering default namespace: default.
    temporal             | {"level":"info","ts":"2026-01-20T03:18:17.328Z","msg":"history client encountered error","service":"frontend","error":"Not enough hosts to serve the request","service-error-type":"serviceerror.Unavailable","logging-call-at":"metric_client.go:104"}
    temporal             | {"level":"error","ts":"2026-01-20T03:18:17.328Z","msg":"service failures","operation":"StartWorkflowExecution","wf-namespace":"temporal-system","error":"Not enough hosts to serve the request","logging-call-at":"telemetry.go:341","stacktrace":"go.temporal.io/server/common/log.(*zapLogger).Error\n\t/home/builder/temporal/common/log/zap_logger.go:156\ngo.temporal.io/server/common/rpc/interceptor.(*TelemetryInterceptor).handleError\n\t/home/builder/temporal/common/rpc/interceptor/telemetry.go:341\ngo.temporal.io/server/common/rpc/interceptor.(*TelemetryInterceptor).UnaryIntercept\n\t/home/builder/temporal/common/rpc/interceptor/telemetry.go:174\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.temporal.io/server/service/frontend.(*RedirectionInterceptor).handleRedirectAPIInvocation.func2\n\t/home/builder/temporal/service/frontend/redirection_interceptor.go:238\ngo.temporal.io/server/service/frontend.(*NoopRedirectionPolicy).WithNamespaceRedirect\n\t/home/builder/temporal/service/frontend/dc_redirection_policy.go:125\ngo.temporal.io/server/service/frontend.(*RedirectionInterceptor).handleRedirectAPIInvocation\n\t/home/builder/temporal/service/frontend/redirection_interceptor.go:235\ngo.temporal.io/server/service/frontend.(*RedirectionInterceptor).Intercept\n\t/home/builder/temporal/service/frontend/redirection_interceptor.go:195\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.temporal.io/server/common/authorization.(*interceptor).Interceptor\n\t/home/builder/temporal/common/authorization/interceptor.go:158\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.temporal.io/server/common/metrics.NewServerMetricsContextInjectorInterceptor.func1\n\t/home/builder/temporal/common/metrics/grpc.go:66\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc.UnaryServerInterceptor.func1\n\t/go/pkg/mod/go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc@v0.42.0/interceptor.go:344\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.temporal.io/server/common/rpc/interceptor.(*NamespaceLogInterceptor).Intercept\n\t/home/builder/temporal/common/rpc/interceptor/namespace_logger.go:84\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.temporal.io/server/common/rpc/interceptor.(*NamespaceValidatorInterceptor).NamespaceValidateIntercept\n\t/home/builder/temporal/common/rpc/interceptor/namespace_validator.go:111\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.temporal.io/server/common/rpc.ServiceErrorInterceptor\n\t/home/builder/temporal/common/rpc/grpc.go:145\ngoogle.golang.org/grpc.chainUnaryInterceptors.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1186\ngo.temporal.io/api/workflowservice/v1._WorkflowService_StartWorkflowExecution_Handler\n\t/go/pkg/mod/go.temporal.io/api@v1.24.1-0.20231003165936-bb03061759c8/workflowservice/v1/service.pb.go:1627\ngoogle.golang.org/grpc.(*Server).processUnaryRPC\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1376\ngoogle.golang.org/grpc.(*Server).handleStream\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1753\ngoogle.golang.org/grpc.(*Server).serveStreams.func1.1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:998"}
    temporal             | Error: Namespace default is not found.
    temporal             | ('export TEMPORAL_CLI_SHOW_STACKS=1' to see stack traces)
    temporal             | Default namespace default not found. Creating...
    temporal             | {"level":"info","ts":"2026-01-20T03:18:17.370Z","msg":"Register namespace succeeded","service":"frontend","wf-namespace":"default","wf-namespace-id":"2a9e695d-1f25-4fee-83a0-80f438915dfa","logging-call-at":"namespace_handler.go:312"}
    temporal             | Namespace default successfully registered.
    temporal             | Default namespace default registration complete.
    temporal             | Error: unable to list search attributes: Namespace default is not found.
    temporal             | ('export TEMPORAL_CLI_SHOW_STACKS=1' to see stack traces)
    temporal             | Waiting for namespace cache to refresh...
    temporal             | {"level":"info","ts":"2026-01-20T03:18:18.072Z","msg":"history client encountered error","service":"frontend","error":"Not enough hosts to serve the request","service-error-type":"serviceerror.Unavailable","logging-call-at":"metric_client.go:104"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:18.118Z","msg":"history client encountered error","service":"frontend","error":"Not enough hosts to serve the request","service-error-type":"serviceerror.Unavailable","logging-call-at":"metric_client.go:104"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:18.195Z","msg":"history client encountered error","service":"frontend","error":"Not enough hosts to serve the request","service-error-type":"serviceerror.Unavailable","logging-call-at":"metric_client.go:104"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:18.240Z","msg":"history client encountered error","service":"frontend","error":"Not enough hosts to serve the request","service-error-type":"serviceerror.Unavailable","logging-call-at":"metric_client.go:104"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:18.297Z","msg":"history client encountered error","service":"frontend","error":"Not enough hosts to serve the request","service-error-type":"serviceerror.Unavailable","logging-call-at":"metric_client.go:104"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:18.342Z","msg":"history client encountered error","service":"frontend","error":"Not enough hosts to serve the request","service-error-type":"serviceerror.Unavailable","logging-call-at":"metric_client.go:104"}
    temporal             | {"level":"error","ts":"2026-01-20T03:18:18.342Z","msg":"service failures","operation":"StartWorkflowExecution","wf-namespace":"temporal-system","error":"Not enough hosts to serve the request","logging-call-at":"telemetry.go:341","stacktrace":"go.temporal.io/server/common/log.(*zapLogger).Error\n\t/home/builder/temporal/common/log/zap_logger.go:156\ngo.temporal.io/server/common/rpc/interceptor.(*TelemetryInterceptor).handleError\n\t/home/builder/temporal/common/rpc/interceptor/telemetry.go:341\ngo.temporal.io/server/common/rpc/interceptor.(*TelemetryInterceptor).UnaryIntercept\n\t/home/builder/temporal/common/rpc/interceptor/telemetry.go:174\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.temporal.io/server/service/frontend.(*RedirectionInterceptor).handleRedirectAPIInvocation.func2\n\t/home/builder/temporal/service/frontend/redirection_interceptor.go:238\ngo.temporal.io/server/service/frontend.(*NoopRedirectionPolicy).WithNamespaceRedirect\n\t/home/builder/temporal/service/frontend/dc_redirection_policy.go:125\ngo.temporal.io/server/service/frontend.(*RedirectionInterceptor).handleRedirectAPIInvocation\n\t/home/builder/temporal/service/frontend/redirection_interceptor.go:235\ngo.temporal.io/server/service/frontend.(*RedirectionInterceptor).Intercept\n\t/home/builder/temporal/service/frontend/redirection_interceptor.go:195\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.temporal.io/server/common/authorization.(*interceptor).Interceptor\n\t/home/builder/temporal/common/authorization/interceptor.go:158\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.temporal.io/server/common/metrics.NewServerMetricsContextInjectorInterceptor.func1\n\t/home/builder/temporal/common/metrics/grpc.go:66\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc.UnaryServerInterceptor.func1\n\t/go/pkg/mod/go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc@v0.42.0/interceptor.go:344\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.temporal.io/server/common/rpc/interceptor.(*NamespaceLogInterceptor).Intercept\n\t/home/builder/temporal/common/rpc/interceptor/namespace_logger.go:84\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.temporal.io/server/common/rpc/interceptor.(*NamespaceValidatorInterceptor).NamespaceValidateIntercept\n\t/home/builder/temporal/common/rpc/interceptor/namespace_validator.go:111\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.temporal.io/server/common/rpc.ServiceErrorInterceptor\n\t/home/builder/temporal/common/rpc/grpc.go:145\ngoogle.golang.org/grpc.chainUnaryInterceptors.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1186\ngo.temporal.io/api/workflowservice/v1._WorkflowService_StartWorkflowExecution_Handler\n\t/go/pkg/mod/go.temporal.io/api@v1.24.1-0.20231003165936-bb03061759c8/workflowservice/v1/service.pb.go:1627\ngoogle.golang.org/grpc.(*Server).processUnaryRPC\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1376\ngoogle.golang.org/grpc.(*Server).handleStream\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1753\ngoogle.golang.org/grpc.(*Server).serveStreams.func1.1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:998"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:18.433Z","msg":"history client encountered error","service":"frontend","error":"Not enough hosts to serve the request","service-error-type":"serviceerror.Unavailable","logging-call-at":"metric_client.go:104"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:18.433Z","msg":"Current reachable members","component":"service-resolver","service":"history","addresses":["172.18.0.3:7234"],"logging-call-at":"service_resolver.go:279"}
    temporal             |   [95m           Name           [0m  [95m   Type    [0m  
    temporal             |   BatcherNamespace            Keyword      
    temporal             |   BatcherUser                 Keyword      
    temporal             |   BinaryChecksums             KeywordList  
    temporal             |   BuildIds                    KeywordList  
    temporal             |   CloseTime                   Datetime     
    temporal             |   ExecutionDuration           Int          
    temporal             |   ExecutionStatus             Keyword      
    temporal             |   ExecutionTime               Datetime     
    temporal             |   HistoryLength               Int          
    temporal             |   HistorySizeBytes            Int          
    temporal             |   RunId                       Keyword      
    temporal             |   StartTime                   Datetime     
    temporal             |   StateTransitionCount        Int          
    temporal             |   TaskQueue                   Keyword      
    temporal             |   TemporalChangeVersion       KeywordList  
    temporal             |   TemporalNamespaceDivision   Keyword      
    temporal             |   TemporalSchedulePaused      Bool         
    temporal             |   TemporalScheduledById       Keyword      
    temporal             |   TemporalScheduledStartTime  Datetime     
    temporal             |   WorkflowId                  Keyword      
    temporal             |   WorkflowType                Keyword      
    temporal             | Namespace cache refreshed.
    temporal             | Adding Custom*Field search attributes.
    temporal             | {"level":"info","ts":"2026-01-20T03:18:18.478Z","msg":"history client encountered error","service":"matching","error":"Not enough hosts to serve the request","service-error-type":"serviceerror.Unavailable","logging-call-at":"metric_client.go:104"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:18.478Z","msg":"Current reachable members","component":"service-resolver","service":"history","addresses":["172.18.0.3:7234"],"logging-call-at":"service_resolver.go:279"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:18.479Z","msg":"temporal-sys-history-scanner-workflow workflow successfully started","service":"worker","logging-call-at":"scanner.go:292"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:18.532Z","msg":"Workflow started.","service":"worker","Namespace":"temporal-system","TaskQueue":"default-worker-tq","WorkerID":"1@8b4da51fa81b@","WorkflowType":"temporal-sys-add-search-attributes-workflow","WorkflowID":"temporal-sys-add-search-attributes-workflow","RunID":"df34da6f-1f25-4d11-84b4-1ecd8828056a","Attempt":1,"wf-type":"temporal-sys-add-search-attributes-workflow","logging-call-at":"value.go:586"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:18.533Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/default-worker-tq/3","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:18.538Z","msg":"Elasticsearch client is not configured. Skipping mapping update.","logging-call-at":"workflow.go:137"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:18.538Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/default-worker-tq/1","wf-task-queue-type":"Activity","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:18.547Z","msg":"Elasticsearch client is not configured. Skipping Elasticsearch status check.","logging-call-at":"workflow.go:174"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:18.557Z","msg":"Search attributes saved to cluster metadata.","es-index":"","logging-call-at":"workflow.go:202"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:18.560Z","msg":"Workflow finished successfully.","service":"worker","Namespace":"temporal-system","TaskQueue":"default-worker-tq","WorkerID":"1@8b4da51fa81b@","WorkflowType":"temporal-sys-add-search-attributes-workflow","WorkflowID":"temporal-sys-add-search-attributes-workflow","RunID":"df34da6f-1f25-4d11-84b4-1ecd8828056a","Attempt":1,"wf-type":"temporal-sys-add-search-attributes-workflow","logging-call-at":"value.go:586"}
    temporal             | Search attributes have been added
    temporal             | {"level":"info","ts":"2026-01-20T03:18:19.810Z","msg":"temporal-sys-tq-scanner-workflow workflow successfully started","service":"worker","logging-call-at":"scanner.go:292"}
     Container temporal Healthy 
     Container agenticorp Starting 
     Container temporal Healthy 
     Container agenticorp-test Starting 
     Container temporal Healthy 
     Container temporal-ui Starting 
     Container temporal-ui Started 
    temporal-ui          | 2026/01/20 03:18:21 Loading config; env=docker,configDir=config
    temporal-ui          | 2026/01/20 03:18:21 Loading config files=[config/docker.yaml]
    temporal-ui          | 2026/01/20 03:18:21 Loading config; env=docker,configDir=config
    temporal-ui          | 2026/01/20 03:18:21 Loading config files=[config/docker.yaml]
     Container agenticorp Started 
    temporal-ui          | Starting UI server...
    temporal-ui          | 
    temporal-ui          |    ____    __
    temporal-ui          |   / __/___/ /  ___
    temporal-ui          |  / _// __/ _ \/ _ \
    temporal-ui          | /___/\__/_//_/\___/ v4.9.0
    temporal-ui          | High performance, minimalist Go web framework
    temporal-ui          | https://echo.labstack.com
    temporal-ui          | ____________________________________O/_______
    temporal-ui          |                                     O\
    temporal-ui          |  http server started on [::]:8080
     Container agenticorp-test Started 
    agenticorp           | 2026/01/20 03:18:21 main.go:72: AgentiCorp API listening on :8080
    agenticorp-test      | ?   github.com/jordanhubbard/agenticorp[no test files]
    agenticorp-test      | ?   github.com/jordanhubbard/agenticorp/cmd/agenticorp[no test files]
    agenticorp-test      | ?   github.com/jordanhubbard/agenticorp/cmd/yaml-lint[no test files]
    agenticorp-test      | ?   github.com/jordanhubbard/agenticorp/examples/worker_demo[no test files]
    agenticorp-test      | === RUN   TestNewBaseAgent
    agenticorp-test      | --- PASS: TestNewBaseAgent (0.00s)
    agenticorp-test      | === RUN   TestBaseAgent_Execute
    agenticorp-test      | --- PASS: TestBaseAgent_Execute (0.00s)
    agenticorp-test      | === RUN   TestBaseAgent_GetCapabilities
    agenticorp-test      | --- PASS: TestBaseAgent_GetCapabilities (0.00s)
    agenticorp-test      | PASS
    agenticorp-test      | ok  github.com/jordanhubbard/agenticorp/internal/agent0.004s
    agenticorp-test      | === RUN   TestCEODecisionApproveClosesParentBead
    agenticorp-test      | --- PASS: TestCEODecisionApproveClosesParentBead (0.00s)
    agenticorp-test      | === RUN   TestCEODecisionDenyReopensAndUnassignsParentBead
    agenticorp-test      | --- PASS: TestCEODecisionDenyReopensAndUnassignsParentBead (0.00s)
    agenticorp-test      | === RUN   TestCEODecisionNeedsMoreInfoReturnsToPriorOwner
    agenticorp-test      | --- PASS: TestCEODecisionNeedsMoreInfoReturnsToPriorOwner (0.00s)
    agenticorp-test      | === RUN   TestGlobalDispatcherDoesNotPanicWithNoProjects
    agenticorp-test      | --- PASS: TestGlobalDispatcherDoesNotPanicWithNoProjects (0.00s)
    agenticorp-test      | PASS
    agenticorp-test      | ok  github.com/jordanhubbard/agenticorp/internal/agenticorp0.011s
    agenticorp-test      | ?   github.com/jordanhubbard/agenticorp/internal/api[no test files]
    agenticorp-test      | ?   github.com/jordanhubbard/agenticorp/internal/beads[no test files]
    agenticorp-test      | ?   github.com/jordanhubbard/agenticorp/internal/config[no test files]
    agenticorp-test      | ?   github.com/jordanhubbard/agenticorp/internal/database[no test files]
    agenticorp-test      | === RUN   TestNewSimpleMaker
    agenticorp-test      | --- PASS: TestNewSimpleMaker (0.00s)
    agenticorp-test      | === RUN   TestSimpleMaker_DecideAgent
    agenticorp-test      | --- PASS: TestSimpleMaker_DecideAgent (0.00s)
    agenticorp-test      | === RUN   TestSimpleMaker_DecideAgent_NoAgents
    agenticorp-test      | --- PASS: TestSimpleMaker_DecideAgent_NoAgents (0.00s)
    agenticorp-test      | === RUN   TestSimpleMaker_EvaluatePriority
    agenticorp-test      | === RUN   TestSimpleMaker_EvaluatePriority/Urgent_task
    agenticorp-test      | === RUN   TestSimpleMaker_EvaluatePriority/Bug_fix_task
    agenticorp-test      | === RUN   TestSimpleMaker_EvaluatePriority/Normal_task
    agenticorp-test      | --- PASS: TestSimpleMaker_EvaluatePriority (0.00s)
    agenticorp-test      |     --- PASS: TestSimpleMaker_EvaluatePriority/Urgent_task (0.00s)
    agenticorp-test      |     --- PASS: TestSimpleMaker_EvaluatePriority/Bug_fix_task (0.00s)
    agenticorp-test      |     --- PASS: TestSimpleMaker_EvaluatePriority/Normal_task (0.00s)
    agenticorp-test      | === RUN   TestSimpleMaker_calculateScore
    agenticorp-test      | --- PASS: TestSimpleMaker_calculateScore (0.00s)
    agenticorp-test      | PASS
    agenticorp-test      | ok  github.com/jordanhubbard/agenticorp/internal/decision0.002s
    agenticorp-test      | ?   github.com/jordanhubbard/agenticorp/internal/dispatch[no test files]
    agenticorp-test      | === RUN   TestNewTaskDispatcher
    agenticorp-test      | --- PASS: TestNewTaskDispatcher (0.00s)
    agenticorp-test      | === RUN   TestTaskDispatcher_RegisterAgent
    agenticorp-test      | --- PASS: TestTaskDispatcher_RegisterAgent (0.00s)
    agenticorp-test      | === RUN   TestTaskDispatcher_GetAvailableAgents
    agenticorp-test      | --- PASS: TestTaskDispatcher_GetAvailableAgents (0.00s)
    agenticorp-test      | === RUN   TestTaskDispatcher_AssignTask
    agenticorp-test      | --- PASS: TestTaskDispatcher_AssignTask (0.00s)
    agenticorp-test      | === RUN   TestTaskDispatcher_AssignTask_NoAgents
    agenticorp-test      | --- PASS: TestTaskDispatcher_AssignTask_NoAgents (0.00s)
    agenticorp-test      | PASS
    agenticorp-test      | ok  github.com/jordanhubbard/agenticorp/internal/dispatcher0.002s
    agenticorp-test      | === RUN   TestKeyManager
    agenticorp-test      | --- PASS: TestKeyManager (0.03s)
    agenticorp-test      | === RUN   TestKeyManagerPersistence
    agenticorp-test      | --- PASS: TestKeyManagerPersistence (0.03s)
    agenticorp-test      | === RUN   TestKeyManagerWrongPassword
    agenticorp-test      | --- PASS: TestKeyManagerWrongPassword (0.03s)
    agenticorp-test      | PASS
    agenticorp-test      | ok  github.com/jordanhubbard/agenticorp/internal/keymanager0.084s
    agenticorp-test      | ?   github.com/jordanhubbard/agenticorp/internal/modelcatalog[no test files]
    agenticorp-test      | ?   github.com/jordanhubbard/agenticorp/internal/models[no test files]
    agenticorp-test      | === RUN   TestLoadQAEngineerPersona
    agenticorp-test      | --- PASS: TestLoadQAEngineerPersona (0.00s)
    agenticorp-test      | === RUN   TestLoadProjectManagerPersona
    agenticorp-test      | --- PASS: TestLoadProjectManagerPersona (0.00s)
    agenticorp-test      | === RUN   TestListPersonas
    agenticorp-test      | --- PASS: TestListPersonas (0.00s)
    agenticorp-test      | PASS
    agenticorp-test      | ok  github.com/jordanhubbard/agenticorp/internal/persona0.002s
    agenticorp-test      | === RUN   TestProjectStateManagement
    agenticorp-test      | --- PASS: TestProjectStateManagement (0.00s)
    agenticorp-test      | === RUN   TestPerpetualProject
    agenticorp-test      | --- PASS: TestPerpetualProject (0.00s)
    agenticorp-test      | === RUN   TestCanClose
    agenticorp-test      | --- PASS: TestCanClose (0.00s)
    agenticorp-test      | === RUN   TestProjectNotFound
    agenticorp-test      | --- PASS: TestProjectNotFound (0.00s)
    agenticorp-test      | === RUN   TestCloseAlreadyClosed
    agenticorp-test      | --- PASS: TestCloseAlreadyClosed (0.00s)
    agenticorp-test      | === RUN   TestReopenNotClosed
    agenticorp-test      | --- PASS: TestReopenNotClosed (0.00s)
    agenticorp-test      | PASS
    agenticorp-test      | ok  github.com/jordanhubbard/agenticorp/internal/project0.002s
    agenticorp-test      | === RUN   TestProviderRegistry
    agenticorp-test      | --- PASS: TestProviderRegistry (0.00s)
    agenticorp-test      | === RUN   TestProviderProtocol
    agenticorp-test      |     registry_test.go:71: OpenAI provider created successfully
    agenticorp-test      | --- PASS: TestProviderProtocol (0.00s)
    agenticorp-test      | === RUN   TestChatCompletionRequest
    agenticorp-test      | --- PASS: TestChatCompletionRequest (0.00s)
    agenticorp-test      | === RUN   TestProviderTypes
    agenticorp-test      | === RUN   TestProviderTypes/OpenAI_type
    agenticorp-test      | === RUN   TestProviderTypes/Anthropic_type
    agenticorp-test      | === RUN   TestProviderTypes/Local_type
    agenticorp-test      | === RUN   TestProviderTypes/Custom_type
    agenticorp-test      | === RUN   TestProviderTypes/Unknown_type
    agenticorp-test      | --- PASS: TestProviderTypes (0.00s)
    agenticorp-test      |     --- PASS: TestProviderTypes/OpenAI_type (0.00s)
    agenticorp-test      |     --- PASS: TestProviderTypes/Anthropic_type (0.00s)
    agenticorp-test      |     --- PASS: TestProviderTypes/Local_type (0.00s)
    agenticorp-test      |     --- PASS: TestProviderTypes/Custom_type (0.00s)
    agenticorp-test      |     --- PASS: TestProviderTypes/Unknown_type (0.00s)
    agenticorp-test      | PASS
    agenticorp-test      | ok  github.com/jordanhubbard/agenticorp/internal/provider0.002s
    temporal             | {"level":"info","ts":"2026-01-20T03:18:26.335Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"8b4da51fa81b:20d927f7-f5ee-455c-b91a-e0b36573a036","wf-task-queue-type":"Workflow","wf-namespace":"default","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:26.336Z","msg":"Started Worker","service":"worker","Namespace":"default","TaskQueue":"temporal-sys-per-ns-tq","WorkerID":"server-worker@1@8b4da51fa81b@default","logging-call-at":"pernamespaceworker.go:483"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:26.336Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"temporal-sys-per-ns-tq","wf-task-queue-type":"Workflow","wf-namespace":"default","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:18:26.337Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"temporal-sys-per-ns-tq","wf-task-queue-type":"Activity","wf-namespace":"default","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    agenticorp-test      | === RUN   TestTemporalManagerCreation
    agenticorp-test      | 2026/01/20 03:18:24 Connected to Temporal server at temporal:7233 (namespace: test-namespace)
    agenticorp-test      | 2026/01/20 03:18:24 Temporal event bus initialized
    agenticorp-test      | 2026/01/20 03:18:24 Temporal worker registered for task queue: test-queue
    agenticorp-test      | 2026/01/20 03:18:24 Stopping Temporal manager...
    agenticorp-test      | 2026/01/20 03:18:24 [Temporal INFO] Stopped Worker [Namespace test-namespace TaskQueue test-queue WorkerID 3239@dd00904f857a@]
    agenticorp-test      | 2026/01/20 03:18:24 Temporal manager stopped
    agenticorp-test      | --- PASS: TestTemporalManagerCreation (0.04s)
    agenticorp-test      | === RUN   TestTemporalManagerWithoutEventBus
    agenticorp-test      | 2026/01/20 03:18:24 Connected to Temporal server at temporal:7233 (namespace: test-namespace)
    agenticorp-test      | 2026/01/20 03:18:24 Temporal worker registered for task queue: test-queue
    agenticorp-test      | 2026/01/20 03:18:24 Stopping Temporal manager...
    agenticorp-test      | 2026/01/20 03:18:24 [Temporal INFO] Stopped Worker [Namespace test-namespace TaskQueue test-queue WorkerID 3239@dd00904f857a@]
    agenticorp-test      | 2026/01/20 03:18:24 Temporal manager stopped
    agenticorp-test      | --- PASS: TestTemporalManagerWithoutEventBus (0.01s)
    agenticorp-test      | === RUN   TestTemporalManagerNilConfig
    agenticorp-test      | --- PASS: TestTemporalManagerNilConfig (0.00s)
    agenticorp-test      | === RUN   TestTemporalClientCreation
    agenticorp-test      | 2026/01/20 03:18:24 Connected to Temporal server at temporal:7233 (namespace: test-namespace)
    agenticorp-test      | 2026/01/20 03:18:24 Temporal worker registered for task queue: test-queue
    agenticorp-test      | 2026/01/20 03:18:24 Stopping Temporal manager...
    agenticorp-test      | 2026/01/20 03:18:24 [Temporal INFO] Stopped Worker [Namespace test-namespace TaskQueue test-queue WorkerID 3239@dd00904f857a@]
    agenticorp-test      | 2026/01/20 03:18:24 Temporal manager stopped
    agenticorp-test      | --- PASS: TestTemporalClientCreation (0.00s)
    agenticorp-test      | === RUN   TestWorkflowStarter
    agenticorp-test      | 2026/01/20 03:18:24 Connected to Temporal server at temporal:7233 (namespace: test-namespace)
    agenticorp-test      | 2026/01/20 03:18:24 Temporal event bus initialized
    agenticorp-test      | 2026/01/20 03:18:24 Temporal worker registered for task queue: test-queue
    agenticorp-test      | 2026/01/20 03:18:24 Starting Temporal worker...
    agenticorp-test      | 2026/01/20 03:18:24 Temporal worker started successfully
    agenticorp-test      | 2026/01/20 03:18:24 Temporal worker error: Namespace test-namespace is not found.
    agenticorp-test      |     manager_test.go:141: Could not start workflow (worker may not be ready): failed to start agent workflow: Namespace test-namespace is not found.
    agenticorp-test      | 2026/01/20 03:18:26 Stopping Temporal manager...
    agenticorp-test      | 2026/01/20 03:18:26 [Temporal INFO] Stopped Worker [Namespace test-namespace TaskQueue test-queue WorkerID 3239@dd00904f857a@]
    agenticorp-test      | 2026/01/20 03:18:26 Temporal manager stopped
    agenticorp-test      | --- SKIP: TestWorkflowStarter (2.01s)
    agenticorp-test      | PASS
    agenticorp-test      | ok  github.com/jordanhubbard/agenticorp/internal/temporal2.072s
    agenticorp-test      | ?   github.com/jordanhubbard/agenticorp/internal/temporal/activities[no test files]
    agenticorp-test      | ?   github.com/jordanhubbard/agenticorp/internal/temporal/client[no test files]
    agenticorp-test      | === RUN   TestEventBusCreation
    agenticorp-test      | 2026/01/20 03:18:24 Connected to Temporal server at temporal:7233 (namespace: test-namespace)
    agenticorp-test      | --- PASS: TestEventBusCreation (0.01s)
    agenticorp-test      | === RUN   TestEventPublishSubscribe
    agenticorp-test      | 2026/01/20 03:18:24 Connected to Temporal server at temporal:7233 (namespace: test-namespace)
    agenticorp-test      | --- PASS: TestEventPublishSubscribe (0.00s)
    agenticorp-test      | === RUN   TestEventFilter
    agenticorp-test      | 2026/01/20 03:18:24 Connected to Temporal server at temporal:7233 (namespace: test-namespace)
    agenticorp-test      | --- PASS: TestEventFilter (1.01s)
    agenticorp-test      | === RUN   TestMultipleSubscribers
    agenticorp-test      | 2026/01/20 03:18:25 Connected to Temporal server at temporal:7233 (namespace: test-namespace)
    agenticorp-test      | --- PASS: TestMultipleSubscribers (0.01s)
    agenticorp-test      | === RUN   TestEventTypes
    agenticorp-test      | 2026/01/20 03:18:25 Connected to Temporal server at temporal:7233 (namespace: test-namespace)
    agenticorp-test      | --- PASS: TestEventTypes (0.01s)
    agenticorp-test      | PASS
    agenticorp-test      | ok  github.com/jordanhubbard/agenticorp/internal/temporal/eventbus1.044s
    agenticorp-test      | ?   github.com/jordanhubbard/agenticorp/internal/temporal/workflows[no test files]
    agenticorp-test      | ?   github.com/jordanhubbard/agenticorp/internal/worker[no test files]
    agenticorp-test      | ?   github.com/jordanhubbard/agenticorp/pkg/config[no test files]
    agenticorp-test      | ?   github.com/jordanhubbard/agenticorp/pkg/models[no test files]
    agenticorp-test      | ?   github.com/jordanhubbard/agenticorp/pkg/secrets[no test files]
    agenticorp-test      | ?   github.com/jordanhubbard/agenticorp/pkg/server[no test files]
    agenticorp-test      | === RUN   TestTaskStatus
    agenticorp-test      | === RUN   TestTaskStatus/Pending_status
    agenticorp-test      | === RUN   TestTaskStatus/Assigned_status
    agenticorp-test      | === RUN   TestTaskStatus/InProgress_status
    agenticorp-test      | === RUN   TestTaskStatus/Completed_status
    agenticorp-test      | === RUN   TestTaskStatus/Failed_status
    agenticorp-test      | --- PASS: TestTaskStatus (0.00s)
    agenticorp-test      |     --- PASS: TestTaskStatus/Pending_status (0.00s)
    agenticorp-test      |     --- PASS: TestTaskStatus/Assigned_status (0.00s)
    agenticorp-test      |     --- PASS: TestTaskStatus/InProgress_status (0.00s)
    agenticorp-test      |     --- PASS: TestTaskStatus/Completed_status (0.00s)
    agenticorp-test      |     --- PASS: TestTaskStatus/Failed_status (0.00s)
    agenticorp-test      | === RUN   TestAgentType
    agenticorp-test      | === RUN   TestAgentType/General_type
    agenticorp-test      | === RUN   TestAgentType/Specialist_type
    agenticorp-test      | === RUN   TestAgentType/Reviewer_type
    agenticorp-test      | --- PASS: TestAgentType (0.00s)
    agenticorp-test      |     --- PASS: TestAgentType/General_type (0.00s)
    agenticorp-test      |     --- PASS: TestAgentType/Specialist_type (0.00s)
    agenticorp-test      |     --- PASS: TestAgentType/Reviewer_type (0.00s)
    agenticorp-test      | === RUN   TestAgentStatus
    agenticorp-test      | === RUN   TestAgentStatus/Idle_status
    agenticorp-test      | === RUN   TestAgentStatus/Busy_status
    agenticorp-test      | === RUN   TestAgentStatus/Offline_status
    agenticorp-test      | --- PASS: TestAgentStatus (0.00s)
    agenticorp-test      |     --- PASS: TestAgentStatus/Idle_status (0.00s)
    agenticorp-test      |     --- PASS: TestAgentStatus/Busy_status (0.00s)
    agenticorp-test      |     --- PASS: TestAgentStatus/Offline_status (0.00s)
    agenticorp-test      | === RUN   TestTask
    agenticorp-test      | --- PASS: TestTask (0.00s)
    agenticorp-test      | === RUN   TestAgent
    agenticorp-test      | --- PASS: TestAgent (0.00s)
    agenticorp-test      | PASS
    agenticorp-test      | ok  github.com/jordanhubbard/agenticorp/pkg/types0.001s
    agenticorp-test      | === RUN   TestWorkerSystemIntegration
    agenticorp-test      | 2026/01/20 03:18:24 Worker worker-agent-1768879104-test-agent-1-1768879104 started for agent test-agent-1 using provider Mock Provider
    agenticorp-test      | 2026/01/20 03:18:24 Spawned worker worker-agent-1768879104-test-agent-1-1768879104 for agent test-agent-1
    agenticorp-test      | 2026/01/20 03:18:24 Spawned agent test-agent-1 with worker using provider mock-provider
    agenticorp-test      | 2026/01/20 03:18:24 Worker worker-agent-1768879104-test-agent-1-1768879104 stopped
    agenticorp-test      | 2026/01/20 03:18:24 Stopped worker for agent agent-1768879104-test-agent-1
    agenticorp-test      | 2026/01/20 03:18:24 Stopped agent test-agent-1
    agenticorp-test      | --- PASS: TestWorkerSystemIntegration (0.00s)
    agenticorp-test      | === RUN   TestMultipleAgentsWorkflow
    agenticorp-test      | 2026/01/20 03:18:24 Worker worker-agent-1768879104-agent-1-1768879104 started for agent agent-1 using provider Test Provider
    agenticorp-test      | 2026/01/20 03:18:24 Spawned worker worker-agent-1768879104-agent-1-1768879104 for agent agent-1
    agenticorp-test      | 2026/01/20 03:18:24 Spawned agent agent-1 with worker using provider test-provider
    agenticorp-test      | 2026/01/20 03:18:24 Worker worker-agent-1768879104-agent-2-1768879104 started for agent agent-2 using provider Test Provider
    agenticorp-test      | 2026/01/20 03:18:24 Spawned worker worker-agent-1768879104-agent-2-1768879104 for agent agent-2
    agenticorp-test      | 2026/01/20 03:18:24 Spawned agent agent-2 with worker using provider test-provider
    agenticorp-test      | 2026/01/20 03:18:24 Worker worker-agent-1768879104-agent-3-1768879104 started for agent agent-3 using provider Test Provider
    agenticorp-test      | 2026/01/20 03:18:24 Spawned worker worker-agent-1768879104-agent-3-1768879104 for agent agent-3
    agenticorp-test      | 2026/01/20 03:18:24 Spawned agent agent-3 with worker using provider test-provider
    agenticorp-test      | 2026/01/20 03:18:24 Worker worker-agent-1768879104-agent-1-1768879104 stopped
    agenticorp-test      | 2026/01/20 03:18:24 Stopped worker for agent agent-1768879104-agent-1
    agenticorp-test      | 2026/01/20 03:18:24 Stopped agent agent-1
    agenticorp-test      | 2026/01/20 03:18:24 Worker worker-agent-1768879104-agent-2-1768879104 stopped
    agenticorp-test      | 2026/01/20 03:18:24 Stopped worker for agent agent-1768879104-agent-2
    agenticorp-test      | 2026/01/20 03:18:24 Stopped agent agent-2
    agenticorp-test      | 2026/01/20 03:18:24 Worker worker-agent-1768879104-agent-3-1768879104 stopped
    agenticorp-test      | 2026/01/20 03:18:24 Stopped worker for agent agent-1768879104-agent-3
    agenticorp-test      | 2026/01/20 03:18:24 Stopped agent agent-3
    agenticorp-test      | --- PASS: TestMultipleAgentsWorkflow (0.00s)
    agenticorp-test      | === RUN   TestProviderRegistry
    agenticorp-test      | --- PASS: TestProviderRegistry (0.00s)
    agenticorp-test      | === RUN   TestWorkerTaskExecution
    agenticorp-test      | 2026/01/20 03:18:24 Worker worker-agent-1768879104-test-1768879104 started for agent test using provider Test
    agenticorp-test      | 2026/01/20 03:18:24 Spawned worker worker-agent-1768879104-test-1768879104 for agent test
    agenticorp-test      | 2026/01/20 03:18:24 Spawned agent test with worker using provider test
    agenticorp-test      | 2026/01/20 03:18:24 Worker worker-agent-1768879104-test-1768879104 stopped
    agenticorp-test      | 2026/01/20 03:18:24 Stopped worker for agent agent-1768879104-test
    agenticorp-test      | 2026/01/20 03:18:24 Stopped agent test
    agenticorp-test      | --- PASS: TestWorkerTaskExecution (0.00s)
    agenticorp-test      | === RUN   TestWorkerPoolLimits
    agenticorp-test      | 2026/01/20 03:18:24 Worker worker-agent-1768879104-agent-1-1768879104 started for agent agent-1 using provider Test
    agenticorp-test      | 2026/01/20 03:18:24 Spawned worker worker-agent-1768879104-agent-1-1768879104 for agent agent-1
    agenticorp-test      | 2026/01/20 03:18:24 Spawned agent agent-1 with worker using provider test
    agenticorp-test      | 2026/01/20 03:18:24 Worker worker-agent-1768879104-agent-2-1768879104 started for agent agent-2 using provider Test
    agenticorp-test      | 2026/01/20 03:18:24 Spawned worker worker-agent-1768879104-agent-2-1768879104 for agent agent-2
    agenticorp-test      | 2026/01/20 03:18:24 Spawned agent agent-2 with worker using provider test
    agenticorp-test      | 2026/01/20 03:18:24 Worker worker-agent-1768879104-agent-1-1768879104 stopped
    agenticorp-test      | 2026/01/20 03:18:24 Worker worker-agent-1768879104-agent-2-1768879104 stopped
    agenticorp-test      | 2026/01/20 03:18:24 Stopped all workers in pool
    agenticorp-test      | 2026/01/20 03:18:24 Stopped all agents and workers
    agenticorp-test      | --- PASS: TestWorkerPoolLimits (0.00s)
    agenticorp-test      | PASS
    agenticorp-test      | ok  github.com/jordanhubbard/agenticorp/tests/integration0.005s
    [Kagenticorp-test exited with code 0
    temporal             | {"level":"info","ts":"2026-01-20T03:19:15.535Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-sys-history-scanner-taskqueue-0/3","wf-task-queue-type":"Activity","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:19:16.541Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-sys-processor-parent-close-policy/3","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:19:16.541Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-archival-tq/3","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:19:16.542Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-sys-batcher-taskqueue/3","wf-task-queue-type":"Activity","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:19:16.543Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-archival-tq/2","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:20:15.549Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-sys-batcher-taskqueue/2","wf-task-queue-type":"Activity","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:20:15.549Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-archival-tq/1","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:21:14.559Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-sys-batcher-taskqueue/2","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:22:14.547Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/default-worker-tq/1","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:22:14.553Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-sys-processor-parent-close-policy/2","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:22:16.545Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-sys-batcher-taskqueue/1","wf-task-queue-type":"Activity","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal-postgresql  | 2026-01-20 03:23:06.200 UTC [54] LOG:  checkpoint starting: time
    temporal             | {"level":"info","ts":"2026-01-20T03:24:13.573Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-sys-batcher-taskqueue/3","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:24:15.543Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-sys-batcher-taskqueue/1","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Stopped","logging-call-at":"task_queue_manager.go:363"}
    temporal-postgresql  | 2026-01-20 03:25:13.484 UTC [54] LOG:  checkpoint complete: wrote 1230 buffers (7.5%); 1 WAL file(s) added, 0 removed, 0 recycled; write=127.204 s, sync=0.038 s, total=127.284 s; sync files=509, longest=0.003 s, average=0.001 s; distance=5782 kB, estimate=5782 kB
    temporal             | {"level":"info","ts":"2026-01-20T03:25:15.556Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-archival-tq/1","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Stopped","logging-call-at":"task_queue_manager.go:363"}
    temporal             | {"level":"info","ts":"2026-01-20T03:25:15.556Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-sys-batcher-taskqueue/2","wf-task-queue-type":"Activity","wf-namespace":"temporal-system","lifecycle":"Stopped","logging-call-at":"task_queue_manager.go:363"}
    temporal             | {"level":"info","ts":"2026-01-20T03:25:16.543Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/default-worker-tq/2","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Stopped","logging-call-at":"task_queue_manager.go:363"}
    temporal             | {"level":"info","ts":"2026-01-20T03:26:10.573Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-sys-batcher-taskqueue/2","wf-task-queue-type":"Activity","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:26:12.571Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-archival-tq/1","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:26:14.569Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-sys-batcher-taskqueue/2","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Stopped","logging-call-at":"task_queue_manager.go:363"}
    temporal             | {"level":"info","ts":"2026-01-20T03:26:15.547Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/default-worker-tq/3","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Stopped","logging-call-at":"task_queue_manager.go:363"}
    temporal             | {"level":"info","ts":"2026-01-20T03:27:11.586Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-sys-batcher-taskqueue/1","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:27:14.564Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-archival-tq/3","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Stopped","logging-call-at":"task_queue_manager.go:363"}
    temporal             | {"level":"info","ts":"2026-01-20T03:27:14.569Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-archival-tq/3","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:27:15.567Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/default-worker-tq/3","wf-task-queue-type":"Activity","wf-namespace":"temporal-system","lifecycle":"Stopped","logging-call-at":"task_queue_manager.go:363"}
    temporal-postgresql  | 2026-01-20 03:28:06.526 UTC [54] LOG:  checkpoint starting: time
    temporal-postgresql  | 2026-01-20 03:28:10.078 UTC [54] LOG:  checkpoint complete: wrote 35 buffers (0.2%); 0 WAL file(s) added, 0 removed, 0 recycled; write=3.538 s, sync=0.006 s, total=3.552 s; sync files=15, longest=0.005 s, average=0.001 s; distance=199 kB, estimate=5224 kB
    temporal             | {"level":"info","ts":"2026-01-20T03:28:10.593Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-sys-batcher-taskqueue/2","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:28:12.579Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/default-worker-tq/3","wf-task-queue-type":"Activity","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:31:10.577Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-archival-tq/3","wf-task-queue-type":"Activity","wf-namespace":"temporal-system","lifecycle":"Stopped","logging-call-at":"task_queue_manager.go:363"}
    temporal             | {"level":"info","ts":"2026-01-20T03:32:11.596Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-sys-batcher-taskqueue/1","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Stopped","logging-call-at":"task_queue_manager.go:363"}
    temporal-postgresql  | 2026-01-20 03:33:06.137 UTC [54] LOG:  checkpoint starting: time
    temporal             | {"level":"info","ts":"2026-01-20T03:33:06.608Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-sys-batcher-taskqueue/1","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:33:07.605Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/default-worker-tq/3","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal-postgresql  | 2026-01-20 03:33:09.671 UTC [54] LOG:  checkpoint complete: wrote 35 buffers (0.2%); 0 WAL file(s) added, 0 removed, 0 recycled; write=3.523 s, sync=0.005 s, total=3.534 s; sync files=15, longest=0.003 s, average=0.001 s; distance=194 kB, estimate=4721 kB
    temporal             | {"level":"info","ts":"2026-01-20T03:33:10.603Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-sys-batcher-taskqueue/2","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Stopped","logging-call-at":"task_queue_manager.go:363"}
    temporal             | {"level":"info","ts":"2026-01-20T03:34:03.621Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-archival-tq/3","wf-task-queue-type":"Activity","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:34:05.614Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-sys-batcher-taskqueue/2","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:34:06.612Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/default-worker-tq/2","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:34:07.601Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-sys-processor-parent-close-policy/1","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Stopped","logging-call-at":"task_queue_manager.go:363"}
    temporal             | {"level":"info","ts":"2026-01-20T03:37:03.624Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-sys-processor-parent-close-policy/1","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal             | {"level":"info","ts":"2026-01-20T03:37:07.613Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-sys-batcher-taskqueue/3","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Stopped","logging-call-at":"task_queue_manager.go:363"}
    temporal             | {"level":"info","ts":"2026-01-20T03:37:08.608Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-archival-tq/1","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Stopped","logging-call-at":"task_queue_manager.go:363"}
    temporal             | {"level":"info","ts":"2026-01-20T03:38:06.631Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-archival-tq/1","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}
    temporal-postgresql  | 2026-01-20 03:38:06.707 UTC [54] LOG:  checkpoint starting: time
    temporal             | {"level":"info","ts":"2026-01-20T03:38:07.616Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/default-worker-tq/3","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Stopped","logging-call-at":"task_queue_manager.go:363"}
    temporal-postgresql  | 2026-01-20 03:38:10.541 UTC [54] LOG:  checkpoint complete: wrote 39 buffers (0.2%); 0 WAL file(s) added, 0 removed, 0 recycled; write=3.822 s, sync=0.005 s, total=3.834 s; sync files=17, longest=0.004 s, average=0.001 s; distance=203 kB, estimate=4269 kB
    temporal             | {"level":"info","ts":"2026-01-20T03:39:02.632Z","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/default-worker-tq/3","wf-task-queue-type":"Workflow","wf-namespace":"temporal-system","lifecycle":"Started","logging-call-at":"task_queue_manager.go:331"}

status: closed
priority: 0
project_id: agenticorp
assigned_to: null
blocked_by: []
blocks: []
parent: null
children: []
tags:
  - p0
  - failure
  - run
created_at: 2026-01-20T03:39:07Z
updated_at: 2026-01-20T03:39:07Z
closed_at: 2026-01-20T16:52:43Z
context:
  source: makefile
  target: run
